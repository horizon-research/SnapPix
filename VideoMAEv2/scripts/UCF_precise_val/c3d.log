+ unset SLURM_PROCID
+ export MASTER_PORT=26368
+ MASTER_PORT=26368
+ export OMP_NUM_THREADS=1
+ OMP_NUM_THREADS=1
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
+ LR=2e-4
+ OUTPUT_DIR=local/UCF/c3d
+ DATA_PATH=../dataset/shared_list/UCF
+ python3 -m torch.distributed.launch --nproc_per_node=1 --master_port=26368 --nnodes=1 run_class_finetuning.py --model c3d --data_set UCF101 --nb_classes 101 --data_root /localdisk2/dataset/mmdataset --data_path ../dataset/shared_list/UCF --log_dir local/UCF/c3d --output_dir local/UCF/c3d --batch_size 32 --num_sample 2 --input_size 112 --short_side_size 112 --save_ckpt_freq 10 --num_frames 16 --opt adamw --lr 2e-4 --clip_grad 1.0 --num_workers 10 --opt_betas 0.9 0.999 --weight_decay 0.001 --layer_decay 1.0 --epochs 200 --test_num_segment 2 --test_num_crop 3 --local-rank 0 --update_freq 1 --warmup_epochs 30 --validation
/opt/conda/lib/python3.10/site-packages/torch/distributed/launch.py:183: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
[2024-11-17 05:14:07,687] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
| distributed init (rank 0): env://, gpu 0
Namespace(batch_size=32, epochs=200, update_freq=1, save_ckpt_freq=10, model='c3d', tubelet_size=2, input_size=112, with_checkpoint=False, drop=0.0, attn_drop_rate=0.0, drop_path=0.1, head_drop_rate=0.0, disable_eval_during_finetuning=False, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=[0.9, 0.999], clip_grad=1.0, momentum=0.9, weight_decay=0.001, weight_decay_end=None, lr=0.0002, layer_decay=1.0, warmup_lr=1e-08, min_lr=1e-06, warmup_epochs=30, warmup_steps=-1, color_jitter=0.4, num_sample=2, aa='rand-m7-n4-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', crop_pct=None, short_side_size=112, test_num_segment=2, test_num_crop=3, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', model_key='model|module', model_prefix='', init_scale=0.001, use_mean_pooling=True, data_path='../dataset/shared_list/UCF', data_root='/localdisk2/dataset/mmdataset', eval_data_path=None, nb_classes=101, imagenet_default_mean_and_std=True, num_segments=1, num_frames=16, sampling_rate=4, sparse_sample=False, data_set='UCF101', fname_tmpl='img_{:05}.jpg', start_idx=1, output_dir='local/UCF/c3d', log_dir='local/UCF/c3d', device='cuda', seed=0, resume='', auto_resume=True, save_ckpt=True, start_epoch=0, eval=False, validation=True, dist_eval=False, num_workers=10, pin_mem=True, world_size=1, dist_on_itp=False, dist_url='env://', enable_deepspeed=False, debug=False, local_rank=0, rank=0, gpu=0, distributed=True, dist_backend='nccl')
Number of the class = 101
Number of the class = 101
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7ff6bbd47460>
Mixup is activated!
/workspace/CodedExposure/VideoMAEv2/coded/coded_AR/models/c3d.py:52: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.
  m.weight = nn.init.kaiming_normal(m.weight, mode='fan_out')
Patch size = (8, 8)
Model = C3D(
  (conv1): Conv3d(1, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
  (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv2): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
  (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv3a): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
  (conv3b): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
  (bn3): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv4a): Conv3d(256, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
  (conv4b): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
  (bn4): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5a): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
  (conv5b): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
  (bn5): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc6): Linear(in_features=8192, out_features=4096, bias=True)
  (bn6): BatchNorm2d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc7): Linear(in_features=4096, out_features=101, bias=True)
)
number of params: 61635941
LR = 0.00002500
Batch size = 32
Update frequent = 1
Number of training examples = 9537
Number of training training per epoch = 298
Param groups = {
  "decay": {
    "weight_decay": 0.001,
    "params": [
      "conv1.weight",
      "conv2.weight",
      "conv3a.weight",
      "conv3b.weight",
      "conv4a.weight",
      "conv4b.weight",
      "conv5a.weight",
      "conv5b.weight",
      "fc6.weight",
      "fc7.weight"
    ],
    "lr_scale": 1.0
  },
  "no_decay": {
    "weight_decay": 0.0,
    "params": [
      "conv1.bias",
      "bn1.weight",
      "bn1.bias",
      "conv2.bias",
      "bn2.weight",
      "bn2.bias",
      "conv3a.bias",
      "conv3b.bias",
      "bn3.weight",
      "bn3.bias",
      "conv4a.bias",
      "conv4b.bias",
      "bn4.weight",
      "bn4.bias",
      "conv5a.bias",
      "conv5b.bias",
      "bn5.weight",
      "bn5.bias",
      "fc6.bias",
      "bn6.weight",
      "bn6.bias",
      "fc7.bias"
    ],
    "lr_scale": 1.0
  }
}
optimizer settings: {'lr': 2.5e-05, 'weight_decay': 0.0, 'eps': 1e-08, 'betas': [0.9, 0.999]}
Use step level LR scheduler!
Set warmup steps = 8940
Set warmup steps = 0
Max WD = 0.0010000, Min WD = 0.0010000
criterion = SoftTargetCrossEntropy()
Auto resume checkpoint: local/UCF/c3d/checkpoint-199.pth
Resume checkpoint local/UCF/c3d/checkpoint-199.pth
With optim & sched!
/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:1374: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).
  warnings.warn(warn_msg)
Val:  [ 0/79]  eta: 0:07:11  loss: 0.8049 (0.8049)  acc1: 75.0000 (75.0000)  acc5: 93.7500 (93.7500)  time: 5.4673 (5.4673 -- 5.4673)  data: 2.5480 (2.5480 -- 2.5480)  max mem: 8208
Val:  [10/79]  eta: 0:00:40  loss: 1.4327 (1.4506)  acc1: 64.5833 (63.6364)  acc5: 87.5000 (86.1742)  time: 0.5899 (0.0965 -- 5.4673)  data: 0.2323 (0.0002 -- 2.5480)  max mem: 8208
Val:  [20/79]  eta: 0:00:21  loss: 1.3788 (1.4176)  acc1: 66.6667 (64.0873)  acc5: 87.5000 (88.6905)  time: 0.1018 (0.0917 -- 0.1110)  data: 0.0006 (0.0002 -- 0.0017)  max mem: 8208
Val:  [30/79]  eta: 0:00:13  loss: 1.5544 (1.5957)  acc1: 56.2500 (58.8038)  acc5: 87.5000 (85.6855)  time: 0.1112 (0.0881 -- 0.3230)  data: 0.0117 (0.0002 -- 0.2229)  max mem: 8208
Val:  [40/79]  eta: 0:00:10  loss: 1.4569 (1.5144)  acc1: 56.2500 (60.9248)  acc5: 89.5833 (86.7378)  time: 0.1647 (0.0881 -- 1.0131)  data: 0.0639 (0.0003 -- 0.9124)  max mem: 8208
Val:  [50/79]  eta: 0:00:07  loss: 1.4569 (1.6083)  acc1: 58.3333 (58.9461)  acc5: 89.5833 (85.2533)  time: 0.2130 (0.0954 -- 1.0131)  data: 0.1093 (0.0002 -- 0.9124)  max mem: 8208
Val:  [60/79]  eta: 0:00:04  loss: 1.4917 (1.5706)  acc1: 58.3333 (60.0410)  acc5: 89.5833 (86.4754)  time: 0.2185 (0.0925 -- 1.0181)  data: 0.1155 (0.0002 -- 0.9082)  max mem: 8208
Val:  [70/79]  eta: 0:00:02  loss: 1.2569 (1.5226)  acc1: 70.8333 (61.7077)  acc5: 93.7500 (87.0012)  time: 0.2196 (0.0925 -- 1.0181)  data: 0.1168 (0.0002 -- 0.9082)  max mem: 8208
Val:  [78/79]  eta: 0:00:00  loss: 1.2677 (1.4930)  acc1: 68.7500 (62.7016)  acc5: 93.7500 (87.2588)  time: 0.2625 (0.0884 -- 1.2578)  data: 0.1018 (0.0002 -- 0.9034)  max mem: 8208
Val: Total time: 0:00:20 (0.2561 s / it)
* Acc@1 62.702 Acc@5 87.259 loss 1.493
3783 val images: Top-1 62.70%, Top-5 87.26%, loss 1.4930
