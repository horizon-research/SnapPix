+ unset SLURM_PROCID
+ export MASTER_PORT=15870
+ MASTER_PORT=15870
+ export OMP_NUM_THREADS=1
+ OMP_NUM_THREADS=1
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
+ LR=2e-4
+ OUTPUT_DIR=local/SSV2/c3d
+ DATA_PATH=../dataset/shared_list/SSV2_Mine
+ python3 -m torch.distributed.launch --nproc_per_node=1 --master_port=15870 --nnodes=1 run_class_finetuning.py --model c3d --data_set SSV2 --nb_classes 174 --data_root /localdisk2/dataset/mmdataset --data_path ../dataset/shared_list/SSV2_Mine --log_dir local/SSV2/c3d --output_dir local/SSV2/c3d --batch_size 32 --num_sample 2 --input_size 112 --short_side_size 112 --save_ckpt_freq 10 --num_frames 16 --opt adamw --lr 2e-4 --clip_grad 1.0 --num_workers 10 --opt_betas 0.9 0.999 --weight_decay 0.001 --layer_decay 1.0 --epochs 100 --test_num_segment 2 --test_num_crop 3 --local-rank 0 --update_freq 4 --warmup_epochs 15 --validation
/opt/conda/lib/python3.10/site-packages/torch/distributed/launch.py:183: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
[2024-11-17 05:24:45,474] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
| distributed init (rank 0): env://, gpu 0
Namespace(batch_size=32, epochs=100, update_freq=4, save_ckpt_freq=10, model='c3d', tubelet_size=2, input_size=112, with_checkpoint=False, drop=0.0, attn_drop_rate=0.0, drop_path=0.1, head_drop_rate=0.0, disable_eval_during_finetuning=False, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=[0.9, 0.999], clip_grad=1.0, momentum=0.9, weight_decay=0.001, weight_decay_end=None, lr=0.0002, layer_decay=1.0, warmup_lr=1e-08, min_lr=1e-06, warmup_epochs=15, warmup_steps=-1, color_jitter=0.4, num_sample=2, aa='rand-m7-n4-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', crop_pct=None, short_side_size=112, test_num_segment=2, test_num_crop=3, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', model_key='model|module', model_prefix='', init_scale=0.001, use_mean_pooling=True, data_path='../dataset/shared_list/SSV2_Mine', data_root='/localdisk2/dataset/mmdataset', eval_data_path=None, nb_classes=174, imagenet_default_mean_and_std=True, num_segments=1, num_frames=16, sampling_rate=4, sparse_sample=False, data_set='SSV2', fname_tmpl='img_{:05}.jpg', start_idx=1, output_dir='local/SSV2/c3d', log_dir='local/SSV2/c3d', device='cuda', seed=0, resume='', auto_resume=True, save_ckpt=True, start_epoch=0, eval=False, validation=True, dist_eval=False, num_workers=10, pin_mem=True, world_size=1, dist_on_itp=False, dist_url='env://', enable_deepspeed=False, debug=False, local_rank=0, rank=0, gpu=0, distributed=True, dist_backend='nccl')
Number of the class = 174
Number of the class = 174
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f98e9f67460>
Mixup is activated!
/workspace/CodedExposure/VideoMAEv2/coded/coded_AR/models/c3d.py:52: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.
  m.weight = nn.init.kaiming_normal(m.weight, mode='fan_out')
Patch size = (8, 8)
Model = C3D(
  (conv1): Conv3d(1, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
  (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv2): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
  (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv3a): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
  (conv3b): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
  (bn3): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv4a): Conv3d(256, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
  (conv4b): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
  (bn4): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5a): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
  (conv5b): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
  (bn5): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc6): Linear(in_features=8192, out_features=4096, bias=True)
  (bn6): BatchNorm2d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc7): Linear(in_features=4096, out_features=174, bias=True)
)
number of params: 61935022
LR = 0.00010000
Batch size = 128
Update frequent = 4
Number of training examples = 168913
Number of training training per epoch = 1319
Param groups = {
  "decay": {
    "weight_decay": 0.001,
    "params": [
      "conv1.weight",
      "conv2.weight",
      "conv3a.weight",
      "conv3b.weight",
      "conv4a.weight",
      "conv4b.weight",
      "conv5a.weight",
      "conv5b.weight",
      "fc6.weight",
      "fc7.weight"
    ],
    "lr_scale": 1.0
  },
  "no_decay": {
    "weight_decay": 0.0,
    "params": [
      "conv1.bias",
      "bn1.weight",
      "bn1.bias",
      "conv2.bias",
      "bn2.weight",
      "bn2.bias",
      "conv3a.bias",
      "conv3b.bias",
      "bn3.weight",
      "bn3.bias",
      "conv4a.bias",
      "conv4b.bias",
      "bn4.weight",
      "bn4.bias",
      "conv5a.bias",
      "conv5b.bias",
      "bn5.weight",
      "bn5.bias",
      "fc6.bias",
      "bn6.weight",
      "bn6.bias",
      "fc7.bias"
    ],
    "lr_scale": 1.0
  }
}
optimizer settings: {'lr': 0.0001, 'weight_decay': 0.0, 'eps': 1e-08, 'betas': [0.9, 0.999]}
Use step level LR scheduler!
Set warmup steps = 19785
Set warmup steps = 0
Max WD = 0.0010000, Min WD = 0.0010000
criterion = SoftTargetCrossEntropy()
Auto resume checkpoint: local/SSV2/c3d/checkpoint-99.pth
Resume checkpoint local/SSV2/c3d/checkpoint-99.pth
With optim & sched!
/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:1374: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).
  warnings.warn(warn_msg)
Val:  [  0/517]  eta: 0:51:15  loss: 2.6721 (2.6721)  acc1: 41.6667 (41.6667)  acc5: 72.9167 (72.9167)  time: 5.9492 (5.9492 -- 5.9492)  data: 3.3486 (3.3486 -- 3.3486)  max mem: 8215
Val:  [ 10/517]  eta: 0:05:16  loss: 2.8222 (2.8197)  acc1: 35.4167 (35.4167)  acc5: 64.5833 (63.6364)  time: 0.6241 (0.0747 -- 5.9492)  data: 0.3048 (0.0002 -- 3.3486)  max mem: 8215
Val:  [ 20/517]  eta: 0:02:54  loss: 2.8525 (2.9357)  acc1: 31.2500 (33.7302)  acc5: 58.3333 (60.4167)  time: 0.0713 (0.0499 -- 0.1019)  data: 0.0005 (0.0002 -- 0.0010)  max mem: 8215
Val:  [ 30/517]  eta: 0:02:15  loss: 2.8827 (2.9224)  acc1: 31.2500 (33.9382)  acc5: 58.3333 (61.8952)  time: 0.0865 (0.0499 -- 0.3341)  data: 0.0276 (0.0003 -- 0.2464)  max mem: 8215
Val:  [ 40/517]  eta: 0:01:59  loss: 2.8827 (2.9120)  acc1: 35.4167 (34.0955)  acc5: 62.5000 (61.7886)  time: 0.1449 (0.0503 -- 0.4658)  data: 0.0771 (0.0003 -- 0.3828)  max mem: 8215
Val:  [ 50/517]  eta: 0:01:50  loss: 2.9101 (2.9245)  acc1: 31.2500 (33.5376)  acc5: 62.5000 (62.0915)  time: 0.1763 (0.0505 -- 0.6474)  data: 0.1047 (0.0004 -- 0.5570)  max mem: 8215
Val:  [ 60/517]  eta: 0:01:42  loss: 2.9096 (2.9274)  acc1: 33.3333 (33.5724)  acc5: 60.4167 (61.5779)  time: 0.1728 (0.0504 -- 0.6474)  data: 0.1019 (0.0003 -- 0.5693)  max mem: 8215
Val:  [ 70/517]  eta: 0:01:38  loss: 2.8568 (2.9219)  acc1: 33.3333 (33.3333)  acc5: 58.3333 (61.5610)  time: 0.1796 (0.0504 -- 0.9006)  data: 0.1069 (0.0003 -- 0.8074)  max mem: 8215
Val:  [ 80/517]  eta: 0:01:34  loss: 2.9451 (2.9244)  acc1: 33.3333 (33.5134)  acc5: 58.3333 (61.4198)  time: 0.1870 (0.0504 -- 0.9006)  data: 0.1079 (0.0003 -- 0.8074)  max mem: 8215
Val:  [ 90/517]  eta: 0:01:30  loss: 2.9451 (2.9272)  acc1: 35.4167 (33.8141)  acc5: 58.3333 (61.1493)  time: 0.1771 (0.0505 -- 0.8980)  data: 0.0978 (0.0003 -- 0.8059)  max mem: 8215
Val:  [100/517]  eta: 0:01:26  loss: 2.8888 (2.9227)  acc1: 35.4167 (33.8490)  acc5: 60.4167 (61.3243)  time: 0.1742 (0.0506 -- 0.8980)  data: 0.0925 (0.0003 -- 0.8059)  max mem: 8215
Val:  [110/517]  eta: 0:01:21  loss: 2.8472 (2.9192)  acc1: 33.3333 (33.7650)  acc5: 62.5000 (61.5240)  time: 0.1527 (0.0511 -- 0.8214)  data: 0.0721 (0.0003 -- 0.7348)  max mem: 8215
Val:  [120/517]  eta: 0:01:18  loss: 2.8157 (2.9117)  acc1: 33.3333 (33.9876)  acc5: 62.5000 (61.6047)  time: 0.1413 (0.0508 -- 0.6342)  data: 0.0642 (0.0002 -- 0.5840)  max mem: 8215
Val:  [130/517]  eta: 0:01:14  loss: 2.8157 (2.9169)  acc1: 33.3333 (33.7945)  acc5: 62.5000 (61.5299)  time: 0.1527 (0.0505 -- 0.5034)  data: 0.0818 (0.0002 -- 0.4530)  max mem: 8215
Val:  [140/517]  eta: 0:01:11  loss: 3.0131 (2.9262)  acc1: 29.1667 (33.5993)  acc5: 58.3333 (61.1407)  time: 0.1507 (0.0505 -- 0.5894)  data: 0.0836 (0.0003 -- 0.5387)  max mem: 8215
Val:  [150/517]  eta: 0:01:08  loss: 3.0423 (2.9374)  acc1: 29.1667 (33.3747)  acc5: 56.2500 (60.7202)  time: 0.1496 (0.0505 -- 0.5989)  data: 0.0850 (0.0003 -- 0.5486)  max mem: 8215
Val:  [160/517]  eta: 0:01:06  loss: 3.0184 (2.9392)  acc1: 29.1667 (33.2557)  acc5: 56.2500 (60.5331)  time: 0.1601 (0.0505 -- 0.8740)  data: 0.1006 (0.0002 -- 0.8233)  max mem: 8215
Val:  [170/517]  eta: 0:01:03  loss: 2.9955 (2.9474)  acc1: 29.1667 (33.1871)  acc5: 54.1667 (60.2583)  time: 0.1528 (0.0504 -- 0.8740)  data: 0.0925 (0.0002 -- 0.8233)  max mem: 8215
Val:  [180/517]  eta: 0:01:01  loss: 2.9209 (2.9419)  acc1: 31.2500 (33.2413)  acc5: 60.4167 (60.3936)  time: 0.1492 (0.0504 -- 0.7586)  data: 0.0868 (0.0003 -- 0.7078)  max mem: 8215
Val:  [190/517]  eta: 0:00:58  loss: 2.8494 (2.9364)  acc1: 33.3333 (33.2897)  acc5: 62.5000 (60.5257)  time: 0.1499 (0.0510 -- 0.7586)  data: 0.0767 (0.0003 -- 0.7078)  max mem: 8215
Val:  [200/517]  eta: 0:00:56  loss: 2.8396 (2.9358)  acc1: 35.4167 (33.2401)  acc5: 62.5000 (60.5618)  time: 0.1453 (0.0507 -- 0.4288)  data: 0.0680 (0.0003 -- 0.3381)  max mem: 8215
Val:  [210/517]  eta: 0:00:54  loss: 2.8411 (2.9285)  acc1: 35.4167 (33.5308)  acc5: 62.5000 (60.6833)  time: 0.1491 (0.0507 -- 0.4410)  data: 0.0763 (0.0003 -- 0.3488)  max mem: 8215
Val:  [220/517]  eta: 0:00:52  loss: 2.8470 (2.9266)  acc1: 35.4167 (33.5879)  acc5: 64.5833 (60.7466)  time: 0.1535 (0.0507 -- 0.4980)  data: 0.0813 (0.0003 -- 0.4053)  max mem: 8215
Val:  [230/517]  eta: 0:00:50  loss: 2.8829 (2.9254)  acc1: 35.4167 (33.6490)  acc5: 62.5000 (60.7774)  time: 0.1498 (0.0507 -- 0.4980)  data: 0.0795 (0.0005 -- 0.4053)  max mem: 8215
Val:  [240/517]  eta: 0:00:48  loss: 2.7954 (2.9226)  acc1: 33.3333 (33.8088)  acc5: 60.4167 (60.7624)  time: 0.1555 (0.0504 -- 0.5465)  data: 0.0882 (0.0003 -- 0.4961)  max mem: 8215
Val:  [250/517]  eta: 0:00:46  loss: 2.9224 (2.9241)  acc1: 33.3333 (33.6985)  acc5: 60.4167 (60.6325)  time: 0.1518 (0.0504 -- 0.5465)  data: 0.0920 (0.0003 -- 0.4961)  max mem: 8215
Val:  [260/517]  eta: 0:00:44  loss: 2.8798 (2.9208)  acc1: 33.3333 (33.7324)  acc5: 60.4167 (60.7439)  time: 0.1400 (0.0507 -- 0.6293)  data: 0.0780 (0.0003 -- 0.5792)  max mem: 8215
Val:  [270/517]  eta: 0:00:42  loss: 2.8349 (2.9212)  acc1: 35.4167 (33.7485)  acc5: 60.4167 (60.8164)  time: 0.1528 (0.0506 -- 0.6293)  data: 0.0799 (0.0003 -- 0.5792)  max mem: 8215
Val:  [280/517]  eta: 0:00:40  loss: 2.8349 (2.9190)  acc1: 33.3333 (33.7856)  acc5: 62.5000 (60.9134)  time: 0.1637 (0.0506 -- 0.7029)  data: 0.0807 (0.0004 -- 0.6208)  max mem: 8215
Val:  [290/517]  eta: 0:00:38  loss: 2.8195 (2.9184)  acc1: 31.2500 (33.8202)  acc5: 62.5000 (60.8963)  time: 0.1622 (0.0505 -- 0.7029)  data: 0.0798 (0.0002 -- 0.6208)  max mem: 8215
Val:  [300/517]  eta: 0:00:36  loss: 2.8709 (2.9210)  acc1: 31.2500 (33.7971)  acc5: 58.3333 (60.8112)  time: 0.1521 (0.0505 -- 0.5822)  data: 0.0769 (0.0002 -- 0.5318)  max mem: 8215
Val:  [310/517]  eta: 0:00:34  loss: 2.9151 (2.9243)  acc1: 31.2500 (33.7554)  acc5: 58.3333 (60.7851)  time: 0.1396 (0.0506 -- 0.5333)  data: 0.0663 (0.0004 -- 0.4832)  max mem: 8215
Val:  [320/517]  eta: 0:00:33  loss: 2.9151 (2.9233)  acc1: 35.4167 (33.7812)  acc5: 62.5000 (60.8191)  time: 0.1491 (0.0502 -- 0.6523)  data: 0.0819 (0.0003 -- 0.6019)  max mem: 8215
Val:  [330/517]  eta: 0:00:31  loss: 3.0027 (2.9254)  acc1: 31.2500 (33.7424)  acc5: 60.4167 (60.7754)  time: 0.1604 (0.0502 -- 0.6523)  data: 0.0936 (0.0003 -- 0.6019)  max mem: 8215
Val:  [340/517]  eta: 0:00:29  loss: 3.0393 (2.9269)  acc1: 31.2500 (33.7243)  acc5: 58.3333 (60.7710)  time: 0.1499 (0.0507 -- 0.5963)  data: 0.0765 (0.0003 -- 0.5461)  max mem: 8215
Val:  [350/517]  eta: 0:00:27  loss: 3.0913 (2.9303)  acc1: 31.2500 (33.5767)  acc5: 58.3333 (60.7134)  time: 0.1484 (0.0504 -- 0.4734)  data: 0.0708 (0.0003 -- 0.3957)  max mem: 8215
Val:  [360/517]  eta: 0:00:26  loss: 2.9529 (2.9313)  acc1: 29.1667 (33.5469)  acc5: 58.3333 (60.6821)  time: 0.1543 (0.0504 -- 0.4734)  data: 0.0688 (0.0002 -- 0.3841)  max mem: 8215
Val:  [370/517]  eta: 0:00:24  loss: 2.9529 (2.9332)  acc1: 31.2500 (33.5355)  acc5: 58.3333 (60.6244)  time: 0.1431 (0.0507 -- 0.4605)  data: 0.0565 (0.0002 -- 0.3662)  max mem: 8215
Val:  [380/517]  eta: 0:00:22  loss: 2.9513 (2.9319)  acc1: 33.3333 (33.5794)  acc5: 58.3333 (60.5971)  time: 0.1359 (0.0506 -- 0.3780)  data: 0.0552 (0.0002 -- 0.2887)  max mem: 8215
Val:  [390/517]  eta: 0:00:20  loss: 2.9513 (2.9348)  acc1: 33.3333 (33.5145)  acc5: 60.4167 (60.5552)  time: 0.1419 (0.0506 -- 0.4372)  data: 0.0625 (0.0003 -- 0.3862)  max mem: 8215
Val:  [400/517]  eta: 0:00:19  loss: 2.9059 (2.9324)  acc1: 33.3333 (33.5671)  acc5: 62.5000 (60.5777)  time: 0.1520 (0.0507 -- 0.4477)  data: 0.0733 (0.0002 -- 0.3970)  max mem: 8215
Val:  [410/517]  eta: 0:00:17  loss: 2.8713 (2.9336)  acc1: 33.3333 (33.5006)  acc5: 62.5000 (60.6194)  time: 0.1565 (0.0507 -- 0.4477)  data: 0.0745 (0.0002 -- 0.3970)  max mem: 8215
Val:  [420/517]  eta: 0:00:15  loss: 2.8753 (2.9325)  acc1: 31.2500 (33.4719)  acc5: 60.4167 (60.5899)  time: 0.1482 (0.0506 -- 0.3899)  data: 0.0698 (0.0003 -- 0.3392)  max mem: 8215
Val:  [430/517]  eta: 0:00:14  loss: 2.8682 (2.9317)  acc1: 33.3333 (33.4493)  acc5: 60.4167 (60.6729)  time: 0.1449 (0.0506 -- 0.3899)  data: 0.0722 (0.0003 -- 0.3392)  max mem: 8215
Val:  [440/517]  eta: 0:00:12  loss: 2.9372 (2.9337)  acc1: 33.3333 (33.4184)  acc5: 60.4167 (60.6293)  time: 0.1470 (0.0506 -- 0.4913)  data: 0.0750 (0.0004 -- 0.4063)  max mem: 8215
Val:  [450/517]  eta: 0:00:10  loss: 2.8940 (2.9321)  acc1: 33.3333 (33.4303)  acc5: 60.4167 (60.7031)  time: 0.1598 (0.0506 -- 0.5161)  data: 0.0946 (0.0004 -- 0.4252)  max mem: 8215
Val:  [460/517]  eta: 0:00:09  loss: 2.7636 (2.9309)  acc1: 31.2500 (33.3785)  acc5: 60.4167 (60.7059)  time: 0.1588 (0.0504 -- 0.5161)  data: 0.0901 (0.0003 -- 0.4252)  max mem: 8215
Val:  [470/517]  eta: 0:00:07  loss: 2.8266 (2.9286)  acc1: 33.3333 (33.4395)  acc5: 60.4167 (60.7484)  time: 0.1475 (0.0504 -- 0.4789)  data: 0.0788 (0.0003 -- 0.4284)  max mem: 8215
Val:  [480/517]  eta: 0:00:05  loss: 2.8035 (2.9260)  acc1: 33.3333 (33.4200)  acc5: 62.5000 (60.8022)  time: 0.1453 (0.0504 -- 0.4789)  data: 0.0834 (0.0002 -- 0.4284)  max mem: 8215
Val:  [490/517]  eta: 0:00:04  loss: 2.9034 (2.9255)  acc1: 31.2500 (33.4309)  acc5: 62.5000 (60.8155)  time: 0.1524 (0.0504 -- 0.4770)  data: 0.0835 (0.0002 -- 0.3806)  max mem: 8215
Val:  [500/517]  eta: 0:00:02  loss: 2.9718 (2.9275)  acc1: 33.3333 (33.4248)  acc5: 60.4167 (60.7660)  time: 0.1492 (0.0508 -- 0.4770)  data: 0.0742 (0.0003 -- 0.3806)  max mem: 8215
Val:  [510/517]  eta: 0:00:01  loss: 2.9718 (2.9264)  acc1: 33.3333 (33.4353)  acc5: 60.4167 (60.7795)  time: 0.1401 (0.0508 -- 0.4292)  data: 0.0621 (0.0002 -- 0.3312)  max mem: 8215
Val:  [516/517]  eta: 0:00:00  loss: 2.8414 (2.9249)  acc1: 33.3333 (33.4786)  acc5: 60.4167 (60.8104)  time: 0.1425 (0.0508 -- 0.4001)  data: 0.0463 (0.0001 -- 0.3216)  max mem: 8215
Val: Total time: 0:01:23 (0.1618 s / it)
* Acc@1 33.479 Acc@5 60.810 loss 2.925
24777 val images: Top-1 33.48%, Top-5 60.81%, loss 2.9249
