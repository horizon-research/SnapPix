+ unset SLURM_PROCID
+ export MASTER_PORT=21883
+ MASTER_PORT=21883
+ export OMP_NUM_THREADS=1
+ OMP_NUM_THREADS=1
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
+ OUTPUT_DIR=local/SSV2/vitb
+ DATA_PATH=../dataset/shared_list/SSV2_Mine
+ python3 -m torch.distributed.launch --nproc_per_node=1 --master_port=21883 --nnodes=1 run_coded_class_finetuning.py --model coded_vit_base_patch8_112 --finetune local/coded_vitb_pt_decor/checkpoint-299.pth --data_set SSV2 --nb_classes 174 --data_root /localdisk2/dataset/mmdataset --data_path ../dataset/shared_list/SSV2_Mine --log_dir local/SSV2/vitb --output_dir local/SSV2/vitb --batch_size 16 --num_sample 2 --input_size 112 --short_side_size 112 --save_ckpt_freq 20 --num_frames 16 --opt adamw --lr 1e-3 --num_workers 12 --opt_betas 0.9 0.999 --weight_decay 0.05 --layer_decay 0.65 --epochs 50 --test_num_segment 2 --test_num_crop 3 --local-rank 0 --update_freq 8 --warmup_epochs 8 --coded_template_folder ./decorrelation_training_wd0_norm_new --coded_type decor_fix --cross_model None --validation
/opt/conda/lib/python3.10/site-packages/torch/distributed/launch.py:183: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
[2024-11-17 05:35:18,131] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
| distributed init (rank 0): env://, gpu 0
Namespace(batch_size=16, epochs=50, update_freq=8, save_ckpt_freq=20, model='coded_vit_base_patch8_112', tubelet_size=1, input_size=112, with_checkpoint=False, drop=0.0, attn_drop_rate=0.0, drop_path=0.1, head_drop_rate=0.0, disable_eval_during_finetuning=False, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=[0.9, 0.999], clip_grad=None, momentum=0.9, weight_decay=0.05, weight_decay_end=None, lr=0.001, layer_decay=0.65, warmup_lr=1e-08, min_lr=1e-06, warmup_epochs=8, warmup_steps=-1, color_jitter=0.4, num_sample=2, aa='rand-m7-n4-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', crop_pct=None, short_side_size=112, test_num_segment=2, test_num_crop=3, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='local/coded_vitb_pt_decor/checkpoint-299.pth', model_key='model|module', model_prefix='', init_scale=0.001, use_mean_pooling=True, data_path='../dataset/shared_list/SSV2_Mine', data_root='/localdisk2/dataset/mmdataset', eval_data_path=None, nb_classes=174, imagenet_default_mean_and_std=True, num_segments=1, num_frames=16, sampling_rate=4, sparse_sample=False, data_set='SSV2', fname_tmpl='img_{:05}.jpg', start_idx=1, output_dir='local/SSV2/vitb', log_dir='local/SSV2/vitb', device='cuda', seed=0, resume='', auto_resume=True, save_ckpt=True, start_epoch=0, eval=False, validation=True, dist_eval=False, num_workers=12, pin_mem=True, world_size=1, dist_on_itp=False, dist_url='env://', enable_deepspeed=False, debug=False, coded_type='decor_fix', coded_template_folder='./decorrelation_training_wd0_norm_new', local_rank=0, cross_model_path='None', rank=0, gpu=0, distributed=True, dist_backend='nccl')
Number of the class = 174
Number of the class = 174
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f4dcbf93760>
Mixup is activated!
loading pattern_path ./decorrelation_training_wd0_norm_new/pattern_4.pth
Patch size = (8, 8)
Load ckpt from local/coded_vitb_pt_decor/checkpoint-299.pth
Load state_dict by model_key = model
Weights of CodedVisionTransformer not initialized from pretrained model: ['fc_norm.weight', 'fc_norm.bias', 'head.weight', 'head.bias']
Weights from pretrained model not used in CodedVisionTransformer: ['mask_token', 'decoder_pos_embed', 'decoder.blocks.0.norm1.weight', 'decoder.blocks.0.norm1.bias', 'decoder.blocks.0.attn.q_bias', 'decoder.blocks.0.attn.v_bias', 'decoder.blocks.0.attn.qkv.weight', 'decoder.blocks.0.attn.proj.weight', 'decoder.blocks.0.attn.proj.bias', 'decoder.blocks.0.norm2.weight', 'decoder.blocks.0.norm2.bias', 'decoder.blocks.0.mlp.fc1.weight', 'decoder.blocks.0.mlp.fc1.bias', 'decoder.blocks.0.mlp.fc2.weight', 'decoder.blocks.0.mlp.fc2.bias', 'decoder.blocks.1.norm1.weight', 'decoder.blocks.1.norm1.bias', 'decoder.blocks.1.attn.q_bias', 'decoder.blocks.1.attn.v_bias', 'decoder.blocks.1.attn.qkv.weight', 'decoder.blocks.1.attn.proj.weight', 'decoder.blocks.1.attn.proj.bias', 'decoder.blocks.1.norm2.weight', 'decoder.blocks.1.norm2.bias', 'decoder.blocks.1.mlp.fc1.weight', 'decoder.blocks.1.mlp.fc1.bias', 'decoder.blocks.1.mlp.fc2.weight', 'decoder.blocks.1.mlp.fc2.bias', 'decoder.blocks.2.norm1.weight', 'decoder.blocks.2.norm1.bias', 'decoder.blocks.2.attn.q_bias', 'decoder.blocks.2.attn.v_bias', 'decoder.blocks.2.attn.qkv.weight', 'decoder.blocks.2.attn.proj.weight', 'decoder.blocks.2.attn.proj.bias', 'decoder.blocks.2.norm2.weight', 'decoder.blocks.2.norm2.bias', 'decoder.blocks.2.mlp.fc1.weight', 'decoder.blocks.2.mlp.fc1.bias', 'decoder.blocks.2.mlp.fc2.weight', 'decoder.blocks.2.mlp.fc2.bias', 'decoder.blocks.3.norm1.weight', 'decoder.blocks.3.norm1.bias', 'decoder.blocks.3.attn.q_bias', 'decoder.blocks.3.attn.v_bias', 'decoder.blocks.3.attn.qkv.weight', 'decoder.blocks.3.attn.proj.weight', 'decoder.blocks.3.attn.proj.bias', 'decoder.blocks.3.norm2.weight', 'decoder.blocks.3.norm2.bias', 'decoder.blocks.3.mlp.fc1.weight', 'decoder.blocks.3.mlp.fc1.bias', 'decoder.blocks.3.mlp.fc2.weight', 'decoder.blocks.3.mlp.fc2.bias', 'decoder.norm.weight', 'decoder.norm.bias', 'decoder.head.weight', 'decoder.head.bias', 'encoder_to_decoder.weight', 'norm.weight', 'norm.bias']
Model = CodedVisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv3d(1, 768, kernel_size=(1, 8, 8), stride=(1, 8, 8))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.00909090880304575)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0181818176060915)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.027272727340459824)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.036363635212183)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.045454543083906174)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.054545458406209946)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.06363636255264282)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0727272778749466)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.08181818574666977)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.09090909361839294)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.10000000149011612)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): Identity()
  (fc_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (head_dropout): Dropout(p=0.0, inplace=False)
  (head): Linear(in_features=768, out_features=174, bias=True)
  (coded_layer): CodedLayer()
)
number of params: 85231534
LR = 0.00050000
Batch size = 128
Update frequent = 8
Number of training examples = 168913
Number of training training per epoch = 1319
Assigned values = [0.003697205891018715, 0.005688009063105715, 0.008750783174008792, 0.013462743344628911, 0.02071191283789063, 0.03186448128906251, 0.049022278906250015, 0.07541889062500001, 0.11602906250000002, 0.17850625000000003, 0.274625, 0.42250000000000004, 0.65, 1.0]
Skip weight decay list:  {'pos_embed', 'cls_token', 'coded_layer.coded_weight'}
Param groups = {
  "layer_0_decay": {
    "weight_decay": 0.05,
    "params": [
      "patch_embed.proj.weight"
    ],
    "lr_scale": 0.003697205891018715
  },
  "layer_0_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "patch_embed.proj.bias"
    ],
    "lr_scale": 0.003697205891018715
  },
  "layer_1_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.0.norm1.weight",
      "blocks.0.norm1.bias",
      "blocks.0.attn.q_bias",
      "blocks.0.attn.v_bias",
      "blocks.0.attn.proj.bias",
      "blocks.0.norm2.weight",
      "blocks.0.norm2.bias",
      "blocks.0.mlp.fc1.bias",
      "blocks.0.mlp.fc2.bias"
    ],
    "lr_scale": 0.005688009063105715
  },
  "layer_1_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.0.attn.qkv.weight",
      "blocks.0.attn.proj.weight",
      "blocks.0.mlp.fc1.weight",
      "blocks.0.mlp.fc2.weight"
    ],
    "lr_scale": 0.005688009063105715
  },
  "layer_2_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.1.norm1.weight",
      "blocks.1.norm1.bias",
      "blocks.1.attn.q_bias",
      "blocks.1.attn.v_bias",
      "blocks.1.attn.proj.bias",
      "blocks.1.norm2.weight",
      "blocks.1.norm2.bias",
      "blocks.1.mlp.fc1.bias",
      "blocks.1.mlp.fc2.bias"
    ],
    "lr_scale": 0.008750783174008792
  },
  "layer_2_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.1.attn.qkv.weight",
      "blocks.1.attn.proj.weight",
      "blocks.1.mlp.fc1.weight",
      "blocks.1.mlp.fc2.weight"
    ],
    "lr_scale": 0.008750783174008792
  },
  "layer_3_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.2.norm1.weight",
      "blocks.2.norm1.bias",
      "blocks.2.attn.q_bias",
      "blocks.2.attn.v_bias",
      "blocks.2.attn.proj.bias",
      "blocks.2.norm2.weight",
      "blocks.2.norm2.bias",
      "blocks.2.mlp.fc1.bias",
      "blocks.2.mlp.fc2.bias"
    ],
    "lr_scale": 0.013462743344628911
  },
  "layer_3_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.2.attn.qkv.weight",
      "blocks.2.attn.proj.weight",
      "blocks.2.mlp.fc1.weight",
      "blocks.2.mlp.fc2.weight"
    ],
    "lr_scale": 0.013462743344628911
  },
  "layer_4_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.3.norm1.weight",
      "blocks.3.norm1.bias",
      "blocks.3.attn.q_bias",
      "blocks.3.attn.v_bias",
      "blocks.3.attn.proj.bias",
      "blocks.3.norm2.weight",
      "blocks.3.norm2.bias",
      "blocks.3.mlp.fc1.bias",
      "blocks.3.mlp.fc2.bias"
    ],
    "lr_scale": 0.02071191283789063
  },
  "layer_4_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.3.attn.qkv.weight",
      "blocks.3.attn.proj.weight",
      "blocks.3.mlp.fc1.weight",
      "blocks.3.mlp.fc2.weight"
    ],
    "lr_scale": 0.02071191283789063
  },
  "layer_5_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.4.norm1.weight",
      "blocks.4.norm1.bias",
      "blocks.4.attn.q_bias",
      "blocks.4.attn.v_bias",
      "blocks.4.attn.proj.bias",
      "blocks.4.norm2.weight",
      "blocks.4.norm2.bias",
      "blocks.4.mlp.fc1.bias",
      "blocks.4.mlp.fc2.bias"
    ],
    "lr_scale": 0.03186448128906251
  },
  "layer_5_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.4.attn.qkv.weight",
      "blocks.4.attn.proj.weight",
      "blocks.4.mlp.fc1.weight",
      "blocks.4.mlp.fc2.weight"
    ],
    "lr_scale": 0.03186448128906251
  },
  "layer_6_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.5.norm1.weight",
      "blocks.5.norm1.bias",
      "blocks.5.attn.q_bias",
      "blocks.5.attn.v_bias",
      "blocks.5.attn.proj.bias",
      "blocks.5.norm2.weight",
      "blocks.5.norm2.bias",
      "blocks.5.mlp.fc1.bias",
      "blocks.5.mlp.fc2.bias"
    ],
    "lr_scale": 0.049022278906250015
  },
  "layer_6_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.5.attn.qkv.weight",
      "blocks.5.attn.proj.weight",
      "blocks.5.mlp.fc1.weight",
      "blocks.5.mlp.fc2.weight"
    ],
    "lr_scale": 0.049022278906250015
  },
  "layer_7_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.6.norm1.weight",
      "blocks.6.norm1.bias",
      "blocks.6.attn.q_bias",
      "blocks.6.attn.v_bias",
      "blocks.6.attn.proj.bias",
      "blocks.6.norm2.weight",
      "blocks.6.norm2.bias",
      "blocks.6.mlp.fc1.bias",
      "blocks.6.mlp.fc2.bias"
    ],
    "lr_scale": 0.07541889062500001
  },
  "layer_7_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.6.attn.qkv.weight",
      "blocks.6.attn.proj.weight",
      "blocks.6.mlp.fc1.weight",
      "blocks.6.mlp.fc2.weight"
    ],
    "lr_scale": 0.07541889062500001
  },
  "layer_8_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.7.norm1.weight",
      "blocks.7.norm1.bias",
      "blocks.7.attn.q_bias",
      "blocks.7.attn.v_bias",
      "blocks.7.attn.proj.bias",
      "blocks.7.norm2.weight",
      "blocks.7.norm2.bias",
      "blocks.7.mlp.fc1.bias",
      "blocks.7.mlp.fc2.bias"
    ],
    "lr_scale": 0.11602906250000002
  },
  "layer_8_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.7.attn.qkv.weight",
      "blocks.7.attn.proj.weight",
      "blocks.7.mlp.fc1.weight",
      "blocks.7.mlp.fc2.weight"
    ],
    "lr_scale": 0.11602906250000002
  },
  "layer_9_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.8.norm1.weight",
      "blocks.8.norm1.bias",
      "blocks.8.attn.q_bias",
      "blocks.8.attn.v_bias",
      "blocks.8.attn.proj.bias",
      "blocks.8.norm2.weight",
      "blocks.8.norm2.bias",
      "blocks.8.mlp.fc1.bias",
      "blocks.8.mlp.fc2.bias"
    ],
    "lr_scale": 0.17850625000000003
  },
  "layer_9_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.8.attn.qkv.weight",
      "blocks.8.attn.proj.weight",
      "blocks.8.mlp.fc1.weight",
      "blocks.8.mlp.fc2.weight"
    ],
    "lr_scale": 0.17850625000000003
  },
  "layer_10_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.9.norm1.weight",
      "blocks.9.norm1.bias",
      "blocks.9.attn.q_bias",
      "blocks.9.attn.v_bias",
      "blocks.9.attn.proj.bias",
      "blocks.9.norm2.weight",
      "blocks.9.norm2.bias",
      "blocks.9.mlp.fc1.bias",
      "blocks.9.mlp.fc2.bias"
    ],
    "lr_scale": 0.274625
  },
  "layer_10_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.9.attn.qkv.weight",
      "blocks.9.attn.proj.weight",
      "blocks.9.mlp.fc1.weight",
      "blocks.9.mlp.fc2.weight"
    ],
    "lr_scale": 0.274625
  },
  "layer_11_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.10.norm1.weight",
      "blocks.10.norm1.bias",
      "blocks.10.attn.q_bias",
      "blocks.10.attn.v_bias",
      "blocks.10.attn.proj.bias",
      "blocks.10.norm2.weight",
      "blocks.10.norm2.bias",
      "blocks.10.mlp.fc1.bias",
      "blocks.10.mlp.fc2.bias"
    ],
    "lr_scale": 0.42250000000000004
  },
  "layer_11_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.10.attn.qkv.weight",
      "blocks.10.attn.proj.weight",
      "blocks.10.mlp.fc1.weight",
      "blocks.10.mlp.fc2.weight"
    ],
    "lr_scale": 0.42250000000000004
  },
  "layer_12_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.11.norm1.weight",
      "blocks.11.norm1.bias",
      "blocks.11.attn.q_bias",
      "blocks.11.attn.v_bias",
      "blocks.11.attn.proj.bias",
      "blocks.11.norm2.weight",
      "blocks.11.norm2.bias",
      "blocks.11.mlp.fc1.bias",
      "blocks.11.mlp.fc2.bias"
    ],
    "lr_scale": 0.65
  },
  "layer_12_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.11.attn.qkv.weight",
      "blocks.11.attn.proj.weight",
      "blocks.11.mlp.fc1.weight",
      "blocks.11.mlp.fc2.weight"
    ],
    "lr_scale": 0.65
  },
  "layer_13_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "fc_norm.weight",
      "fc_norm.bias",
      "head.bias",
      "coded_layer.coded_weight"
    ],
    "lr_scale": 1.0
  },
  "layer_13_decay": {
    "weight_decay": 0.05,
    "params": [
      "head.weight"
    ],
    "lr_scale": 1.0
  }
}
optimizer settings: {'lr': 0.0005, 'weight_decay': 0.0, 'eps': 1e-08, 'betas': [0.9, 0.999]}
Use step level LR scheduler!
Set warmup steps = 10552
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
criterion = SoftTargetCrossEntropy()
Auto resume checkpoint: local/SSV2/vitb/checkpoint-49.pth
Resume checkpoint local/SSV2/vitb/checkpoint-49.pth
With optim & sched!
Val:  [   0/1033]  eta: 1:06:34  loss: 2.3447 (2.3447)  acc1: 41.6667 (41.6667)  acc5: 70.8333 (70.8333)  time: 3.8673 (3.8673 -- 3.8673)  data: 2.4686 (2.4686 -- 2.4686)  max mem: 1614
Val:  [  10/1033]  eta: 0:06:27  loss: 2.3541 (2.2582)  acc1: 50.0000 (49.2424)  acc5: 70.8333 (75.0000)  time: 0.3790 (0.0233 -- 3.8673)  data: 0.2251 (0.0001 -- 2.4686)  max mem: 1614
Val:  [  20/1033]  eta: 0:03:35  loss: 2.4468 (2.3764)  acc1: 45.8333 (45.4365)  acc5: 70.8333 (72.6191)  time: 0.0305 (0.0217 -- 0.0380)  data: 0.0008 (0.0001 -- 0.0036)  max mem: 1614
Val:  [  30/1033]  eta: 0:02:43  loss: 2.6605 (2.4544)  acc1: 41.6667 (43.8172)  acc5: 70.8333 (71.3710)  time: 0.0440 (0.0171 -- 0.2419)  data: 0.0168 (0.0003 -- 0.2207)  max mem: 1614
Val:  [  40/1033]  eta: 0:02:25  loss: 2.4844 (2.4476)  acc1: 37.5000 (43.2927)  acc5: 70.8333 (71.4431)  time: 0.0761 (0.0154 -- 0.3901)  data: 0.0531 (0.0001 -- 0.3579)  max mem: 1614
Val:  [  50/1033]  eta: 0:02:06  loss: 2.4063 (2.4287)  acc1: 45.8333 (43.7909)  acc5: 70.8333 (71.8137)  time: 0.0765 (0.0150 -- 0.3901)  data: 0.0550 (0.0001 -- 0.3579)  max mem: 1614
Val:  [  60/1033]  eta: 0:01:55  loss: 2.4063 (2.4440)  acc1: 45.8333 (43.7158)  acc5: 70.8333 (71.5847)  time: 0.0624 (0.0150 -- 0.2663)  data: 0.0414 (0.0002 -- 0.2497)  max mem: 1614
Val:  [  70/1033]  eta: 0:01:48  loss: 2.2250 (2.4014)  acc1: 45.8333 (44.5423)  acc5: 75.0000 (72.3005)  time: 0.0706 (0.0153 -- 0.3058)  data: 0.0491 (0.0002 -- 0.2742)  max mem: 1614
Val:  [  80/1033]  eta: 0:01:44  loss: 2.1760 (2.3883)  acc1: 50.0000 (44.9588)  acc5: 75.0000 (72.4794)  time: 0.0808 (0.0153 -- 0.3058)  data: 0.0578 (0.0002 -- 0.2742)  max mem: 1614
Val:  [  90/1033]  eta: 0:01:39  loss: 2.1760 (2.3706)  acc1: 45.8333 (45.0549)  acc5: 75.0000 (73.0769)  time: 0.0823 (0.0153 -- 0.2576)  data: 0.0567 (0.0003 -- 0.2273)  max mem: 1614
Val:  [ 100/1033]  eta: 0:01:35  loss: 2.4200 (2.3847)  acc1: 41.6667 (44.6370)  acc5: 75.0000 (72.9373)  time: 0.0716 (0.0180 -- 0.2576)  data: 0.0451 (0.0003 -- 0.2359)  max mem: 1614
Val:  [ 110/1033]  eta: 0:01:30  loss: 2.2224 (2.3594)  acc1: 45.8333 (45.3078)  acc5: 75.0000 (73.5360)  time: 0.0612 (0.0207 -- 0.2778)  data: 0.0323 (0.0003 -- 0.2359)  max mem: 1614
Val:  [ 120/1033]  eta: 0:01:27  loss: 2.2224 (2.3755)  acc1: 45.8333 (44.9036)  acc5: 75.0000 (73.0372)  time: 0.0672 (0.0255 -- 0.2960)  data: 0.0348 (0.0003 -- 0.2579)  max mem: 1614
Val:  [ 130/1033]  eta: 0:01:25  loss: 2.2948 (2.3567)  acc1: 45.8333 (45.3244)  acc5: 75.0000 (73.4733)  time: 0.0766 (0.0217 -- 0.2960)  data: 0.0468 (0.0003 -- 0.2579)  max mem: 1614
Val:  [ 140/1033]  eta: 0:01:23  loss: 2.2929 (2.3678)  acc1: 45.8333 (44.9173)  acc5: 75.0000 (73.1383)  time: 0.0803 (0.0217 -- 0.2880)  data: 0.0510 (0.0003 -- 0.2610)  max mem: 1614
Val:  [ 150/1033]  eta: 0:01:20  loss: 2.4759 (2.3688)  acc1: 41.6667 (45.0607)  acc5: 66.6667 (72.9029)  time: 0.0724 (0.0211 -- 0.2880)  data: 0.0413 (0.0001 -- 0.2665)  max mem: 1614
Val:  [ 160/1033]  eta: 0:01:18  loss: 2.4791 (2.3774)  acc1: 45.8333 (45.0311)  acc5: 66.6667 (72.7485)  time: 0.0613 (0.0211 -- 0.2879)  data: 0.0293 (0.0001 -- 0.2665)  max mem: 1614
Val:  [ 170/1033]  eta: 0:01:17  loss: 2.4733 (2.3727)  acc1: 45.8333 (45.2729)  acc5: 75.0000 (73.0020)  time: 0.0728 (0.0265 -- 0.3290)  data: 0.0407 (0.0002 -- 0.2977)  max mem: 1614
Val:  [ 180/1033]  eta: 0:01:15  loss: 2.4347 (2.3813)  acc1: 45.8333 (45.1197)  acc5: 70.8333 (72.6519)  time: 0.0759 (0.0184 -- 0.3290)  data: 0.0470 (0.0002 -- 0.2977)  max mem: 1614
Val:  [ 190/1033]  eta: 0:01:14  loss: 2.3330 (2.3802)  acc1: 41.6667 (45.0044)  acc5: 70.8333 (72.7312)  time: 0.0745 (0.0184 -- 0.2951)  data: 0.0439 (0.0002 -- 0.2672)  max mem: 1614
Val:  [ 200/1033]  eta: 0:01:11  loss: 2.1428 (2.3780)  acc1: 45.8333 (45.0663)  acc5: 75.0000 (72.8648)  time: 0.0649 (0.0171 -- 0.3020)  data: 0.0364 (0.0001 -- 0.2678)  max mem: 1614
Val:  [ 210/1033]  eta: 0:01:10  loss: 2.2556 (2.3749)  acc1: 45.8333 (45.1224)  acc5: 75.0000 (73.0648)  time: 0.0673 (0.0158 -- 0.4778)  data: 0.0442 (0.0001 -- 0.4480)  max mem: 1614
Val:  [ 220/1033]  eta: 0:01:08  loss: 2.2628 (2.3687)  acc1: 50.0000 (45.1735)  acc5: 75.0000 (73.1900)  time: 0.0609 (0.0158 -- 0.4778)  data: 0.0336 (0.0001 -- 0.4480)  max mem: 1614
Val:  [ 230/1033]  eta: 0:01:07  loss: 2.2628 (2.3688)  acc1: 41.6667 (45.1479)  acc5: 75.0000 (73.2684)  time: 0.0594 (0.0169 -- 0.4138)  data: 0.0311 (0.0002 -- 0.3973)  max mem: 1614
Val:  [ 240/1033]  eta: 0:01:06  loss: 2.2756 (2.3680)  acc1: 45.8333 (45.1418)  acc5: 75.0000 (73.2192)  time: 0.0857 (0.0169 -- 0.4138)  data: 0.0559 (0.0003 -- 0.3973)  max mem: 1614
Val:  [ 250/1033]  eta: 0:01:05  loss: 2.3365 (2.3656)  acc1: 45.8333 (45.0199)  acc5: 75.0000 (73.2736)  time: 0.0874 (0.0186 -- 0.3428)  data: 0.0574 (0.0002 -- 0.3200)  max mem: 1614
Val:  [ 260/1033]  eta: 0:01:04  loss: 2.4175 (2.3691)  acc1: 41.6667 (45.0511)  acc5: 70.8333 (73.2759)  time: 0.0705 (0.0186 -- 0.3372)  data: 0.0438 (0.0002 -- 0.3200)  max mem: 1614
Val:  [ 270/1033]  eta: 0:01:02  loss: 2.4278 (2.3723)  acc1: 41.6667 (45.0338)  acc5: 75.0000 (73.2626)  time: 0.0648 (0.0160 -- 0.3525)  data: 0.0405 (0.0002 -- 0.3333)  max mem: 1614
Val:  [ 280/1033]  eta: 0:01:01  loss: 2.4278 (2.3795)  acc1: 41.6667 (44.9585)  acc5: 70.8333 (73.1168)  time: 0.0684 (0.0160 -- 0.3525)  data: 0.0412 (0.0001 -- 0.3333)  max mem: 1614
Val:  [ 290/1033]  eta: 0:01:00  loss: 2.3680 (2.3779)  acc1: 41.6667 (44.9742)  acc5: 70.8333 (73.1386)  time: 0.0646 (0.0169 -- 0.3687)  data: 0.0365 (0.0001 -- 0.3519)  max mem: 1614
Val:  [ 300/1033]  eta: 0:00:59  loss: 2.1781 (2.3776)  acc1: 50.0000 (45.0443)  acc5: 75.0000 (73.1174)  time: 0.0721 (0.0166 -- 0.3687)  data: 0.0481 (0.0001 -- 0.3519)  max mem: 1614
Val:  [ 310/1033]  eta: 0:00:58  loss: 2.3993 (2.3835)  acc1: 41.6667 (44.9223)  acc5: 70.8333 (73.0573)  time: 0.0799 (0.0166 -- 0.3348)  data: 0.0511 (0.0001 -- 0.3057)  max mem: 1614
Val:  [ 320/1033]  eta: 0:00:57  loss: 2.3895 (2.3827)  acc1: 41.6667 (44.9507)  acc5: 70.8333 (73.0919)  time: 0.0624 (0.0152 -- 0.3240)  data: 0.0349 (0.0002 -- 0.2871)  max mem: 1614
Val:  [ 330/1033]  eta: 0:00:56  loss: 2.4495 (2.3897)  acc1: 41.6667 (44.8011)  acc5: 70.8333 (72.9733)  time: 0.0631 (0.0152 -- 0.3705)  data: 0.0345 (0.0002 -- 0.3295)  max mem: 1614
Val:  [ 340/1033]  eta: 0:00:54  loss: 2.4773 (2.3913)  acc1: 37.5000 (44.7336)  acc5: 66.6667 (72.8739)  time: 0.0617 (0.0166 -- 0.3705)  data: 0.0323 (0.0001 -- 0.3295)  max mem: 1614
Val:  [ 350/1033]  eta: 0:00:53  loss: 2.1663 (2.3840)  acc1: 45.8333 (44.9193)  acc5: 70.8333 (72.9938)  time: 0.0540 (0.0150 -- 0.3347)  data: 0.0329 (0.0001 -- 0.3138)  max mem: 1614
Val:  [ 360/1033]  eta: 0:00:52  loss: 2.0049 (2.3782)  acc1: 50.0000 (44.9792)  acc5: 79.1667 (73.1533)  time: 0.0707 (0.0150 -- 0.4206)  data: 0.0459 (0.0002 -- 0.3781)  max mem: 1614
Val:  [ 370/1033]  eta: 0:00:51  loss: 2.1265 (2.3820)  acc1: 45.8333 (44.8787)  acc5: 75.0000 (73.0683)  time: 0.0724 (0.0173 -- 0.4206)  data: 0.0432 (0.0002 -- 0.3781)  max mem: 1614
Val:  [ 380/1033]  eta: 0:00:51  loss: 2.2961 (2.3807)  acc1: 45.8333 (44.9256)  acc5: 70.8333 (73.0752)  time: 0.0773 (0.0182 -- 0.4037)  data: 0.0508 (0.0003 -- 0.3752)  max mem: 1614
Val:  [ 390/1033]  eta: 0:00:50  loss: 2.2961 (2.3793)  acc1: 45.8333 (44.9169)  acc5: 75.0000 (73.1032)  time: 0.0862 (0.0182 -- 0.5661)  data: 0.0575 (0.0002 -- 0.5464)  max mem: 1614
Val:  [ 400/1033]  eta: 0:00:49  loss: 2.2185 (2.3737)  acc1: 45.8333 (44.9917)  acc5: 79.1667 (73.2544)  time: 0.0600 (0.0222 -- 0.5661)  data: 0.0287 (0.0002 -- 0.5464)  max mem: 1614
Val:  [ 410/1033]  eta: 0:00:48  loss: 2.1421 (2.3709)  acc1: 45.8333 (44.9919)  acc5: 79.1667 (73.3171)  time: 0.0474 (0.0202 -- 0.3794)  data: 0.0181 (0.0002 -- 0.3475)  max mem: 1614
Val:  [ 420/1033]  eta: 0:00:47  loss: 2.2284 (2.3661)  acc1: 45.8333 (45.1405)  acc5: 70.8333 (73.2878)  time: 0.0695 (0.0189 -- 0.3794)  data: 0.0419 (0.0002 -- 0.3475)  max mem: 1614
Val:  [ 430/1033]  eta: 0:00:46  loss: 2.3982 (2.3683)  acc1: 54.1667 (45.1759)  acc5: 70.8333 (73.2695)  time: 0.0775 (0.0179 -- 0.3663)  data: 0.0499 (0.0001 -- 0.3375)  max mem: 1614
Val:  [ 440/1033]  eta: 0:00:45  loss: 2.4406 (2.3701)  acc1: 41.6667 (45.1531)  acc5: 66.6667 (73.1954)  time: 0.0727 (0.0179 -- 0.3343)  data: 0.0426 (0.0001 -- 0.3044)  max mem: 1614
Val:  [ 450/1033]  eta: 0:00:44  loss: 2.3912 (2.3642)  acc1: 45.8333 (45.2882)  acc5: 70.8333 (73.2908)  time: 0.0700 (0.0169 -- 0.3423)  data: 0.0382 (0.0002 -- 0.3100)  max mem: 1614
Val:  [ 460/1033]  eta: 0:00:43  loss: 2.1711 (2.3640)  acc1: 45.8333 (45.3182)  acc5: 75.0000 (73.2556)  time: 0.0672 (0.0169 -- 0.3680)  data: 0.0390 (0.0002 -- 0.3407)  max mem: 1614
Val:  [ 470/1033]  eta: 0:00:42  loss: 2.3593 (2.3646)  acc1: 41.6667 (45.2849)  acc5: 70.8333 (73.1865)  time: 0.0525 (0.0174 -- 0.3680)  data: 0.0233 (0.0003 -- 0.3407)  max mem: 1614
Val:  [ 480/1033]  eta: 0:00:41  loss: 2.2903 (2.3612)  acc1: 45.8333 (45.3656)  acc5: 75.0000 (73.3108)  time: 0.0604 (0.0161 -- 0.4946)  data: 0.0295 (0.0003 -- 0.4617)  max mem: 1614
Val:  [ 490/1033]  eta: 0:00:41  loss: 2.2903 (2.3606)  acc1: 50.0000 (45.4345)  acc5: 75.0000 (73.2519)  time: 0.0770 (0.0161 -- 0.5139)  data: 0.0489 (0.0002 -- 0.4815)  max mem: 1614
Val:  [ 500/1033]  eta: 0:00:40  loss: 2.4886 (2.3634)  acc1: 45.8333 (45.4092)  acc5: 70.8333 (73.2119)  time: 0.0781 (0.0200 -- 0.5139)  data: 0.0471 (0.0002 -- 0.4815)  max mem: 1614
Val:  [ 510/1033]  eta: 0:00:39  loss: 2.4271 (2.3637)  acc1: 41.6667 (45.4175)  acc5: 70.8333 (73.2306)  time: 0.0711 (0.0227 -- 0.4856)  data: 0.0400 (0.0003 -- 0.4309)  max mem: 1614
Val:  [ 520/1033]  eta: 0:00:38  loss: 2.4271 (2.3645)  acc1: 45.8333 (45.3775)  acc5: 75.0000 (73.2486)  time: 0.0650 (0.0227 -- 0.3788)  data: 0.0364 (0.0003 -- 0.3552)  max mem: 1614
Val:  [ 530/1033]  eta: 0:00:37  loss: 2.1465 (2.3603)  acc1: 45.8333 (45.3939)  acc5: 75.0000 (73.3286)  time: 0.0509 (0.0244 -- 0.3576)  data: 0.0211 (0.0003 -- 0.3270)  max mem: 1614
Val:  [ 540/1033]  eta: 0:00:36  loss: 2.2384 (2.3644)  acc1: 45.8333 (45.3173)  acc5: 75.0000 (73.2286)  time: 0.0597 (0.0172 -- 0.5414)  data: 0.0284 (0.0003 -- 0.5074)  max mem: 1614
Val:  [ 550/1033]  eta: 0:00:36  loss: 2.3393 (2.3622)  acc1: 41.6667 (45.3191)  acc5: 70.8333 (73.2834)  time: 0.0735 (0.0172 -- 0.5414)  data: 0.0420 (0.0001 -- 0.5074)  max mem: 1614
Val:  [ 560/1033]  eta: 0:00:35  loss: 2.2883 (2.3626)  acc1: 45.8333 (45.3951)  acc5: 75.0000 (73.2992)  time: 0.0724 (0.0202 -- 0.4389)  data: 0.0392 (0.0001 -- 0.4063)  max mem: 1614
Val:  [ 570/1033]  eta: 0:00:34  loss: 2.3388 (2.3644)  acc1: 45.8333 (45.3809)  acc5: 75.0000 (73.2341)  time: 0.0797 (0.0241 -- 0.4389)  data: 0.0458 (0.0003 -- 0.4063)  max mem: 1614
Val:  [ 580/1033]  eta: 0:00:33  loss: 2.2659 (2.3638)  acc1: 45.8333 (45.4174)  acc5: 75.0000 (73.2501)  time: 0.0750 (0.0283 -- 0.4349)  data: 0.0435 (0.0003 -- 0.4036)  max mem: 1614
Val:  [ 590/1033]  eta: 0:00:32  loss: 2.4092 (2.3678)  acc1: 50.0000 (45.4033)  acc5: 66.6667 (73.0823)  time: 0.0528 (0.0255 -- 0.3893)  data: 0.0228 (0.0003 -- 0.3564)  max mem: 1614
Val:  [ 600/1033]  eta: 0:00:32  loss: 2.4447 (2.3686)  acc1: 45.8333 (45.3411)  acc5: 66.6667 (73.0727)  time: 0.0600 (0.0255 -- 0.5790)  data: 0.0298 (0.0002 -- 0.5455)  max mem: 1614
Val:  [ 610/1033]  eta: 0:00:31  loss: 2.4019 (2.3705)  acc1: 45.8333 (45.3082)  acc5: 75.0000 (73.0360)  time: 0.0806 (0.0222 -- 0.5790)  data: 0.0508 (0.0002 -- 0.5455)  max mem: 1614
Val:  [ 620/1033]  eta: 0:00:30  loss: 2.4083 (2.3724)  acc1: 45.8333 (45.2697)  acc5: 70.8333 (73.0072)  time: 0.0795 (0.0216 -- 0.4880)  data: 0.0499 (0.0003 -- 0.4615)  max mem: 1614
Val:  [ 630/1033]  eta: 0:00:29  loss: 2.3195 (2.3717)  acc1: 41.6667 (45.2919)  acc5: 70.8333 (72.9992)  time: 0.0739 (0.0216 -- 0.4291)  data: 0.0409 (0.0002 -- 0.3977)  max mem: 1614
Val:  [ 640/1033]  eta: 0:00:29  loss: 2.2850 (2.3738)  acc1: 41.6667 (45.2483)  acc5: 70.8333 (72.9914)  time: 0.0859 (0.0198 -- 0.5538)  data: 0.0533 (0.0002 -- 0.5331)  max mem: 1614
Val:  [ 650/1033]  eta: 0:00:28  loss: 2.4133 (2.3747)  acc1: 41.6667 (45.2317)  acc5: 70.8333 (72.9583)  time: 0.0729 (0.0198 -- 0.5538)  data: 0.0418 (0.0002 -- 0.5331)  max mem: 1614
Val:  [ 660/1033]  eta: 0:00:27  loss: 2.4494 (2.3774)  acc1: 41.6667 (45.1399)  acc5: 70.8333 (72.9766)  time: 0.0554 (0.0193 -- 0.4595)  data: 0.0246 (0.0002 -- 0.4340)  max mem: 1614
Val:  [ 670/1033]  eta: 0:00:26  loss: 2.3020 (2.3747)  acc1: 41.6667 (45.2062)  acc5: 75.0000 (73.0253)  time: 0.0743 (0.0193 -- 0.4779)  data: 0.0442 (0.0002 -- 0.4394)  max mem: 1614
Val:  [ 680/1033]  eta: 0:00:26  loss: 2.2001 (2.3750)  acc1: 45.8333 (45.2704)  acc5: 75.0000 (73.0237)  time: 0.0764 (0.0213 -- 0.4779)  data: 0.0426 (0.0002 -- 0.4394)  max mem: 1614
Val:  [ 690/1033]  eta: 0:00:25  loss: 2.4973 (2.3772)  acc1: 41.6667 (45.2424)  acc5: 70.8333 (72.9559)  time: 0.0790 (0.0213 -- 0.4555)  data: 0.0422 (0.0002 -- 0.4205)  max mem: 1614
Val:  [ 700/1033]  eta: 0:00:24  loss: 2.3586 (2.3779)  acc1: 41.6667 (45.2389)  acc5: 70.8333 (72.9434)  time: 0.0765 (0.0218 -- 0.4555)  data: 0.0405 (0.0002 -- 0.4205)  max mem: 1614
Val:  [ 710/1033]  eta: 0:00:23  loss: 2.3336 (2.3799)  acc1: 45.8333 (45.1770)  acc5: 75.0000 (72.9255)  time: 0.0542 (0.0218 -- 0.3963)  data: 0.0192 (0.0003 -- 0.3542)  max mem: 1614
Val:  [ 720/1033]  eta: 0:00:23  loss: 2.3208 (2.3804)  acc1: 41.6667 (45.1514)  acc5: 75.0000 (72.9138)  time: 0.0568 (0.0226 -- 0.4815)  data: 0.0223 (0.0002 -- 0.4264)  max mem: 1614
Val:  [ 730/1033]  eta: 0:00:22  loss: 2.4700 (2.3823)  acc1: 41.6667 (45.1379)  acc5: 70.8333 (72.8739)  time: 0.0798 (0.0203 -- 0.4856)  data: 0.0455 (0.0002 -- 0.4567)  max mem: 1614
Val:  [ 740/1033]  eta: 0:00:21  loss: 2.4944 (2.3848)  acc1: 41.6667 (45.1642)  acc5: 66.6667 (72.7564)  time: 0.0813 (0.0189 -- 0.4962)  data: 0.0475 (0.0003 -- 0.4647)  max mem: 1614
Val:  [ 750/1033]  eta: 0:00:20  loss: 2.5428 (2.3872)  acc1: 45.8333 (45.1398)  acc5: 62.5000 (72.6975)  time: 0.0738 (0.0189 -- 0.4962)  data: 0.0406 (0.0003 -- 0.4647)  max mem: 1614
Val:  [ 760/1033]  eta: 0:00:20  loss: 2.4705 (2.3857)  acc1: 45.8333 (45.1873)  acc5: 66.6667 (72.6949)  time: 0.0701 (0.0159 -- 0.4528)  data: 0.0381 (0.0001 -- 0.4199)  max mem: 1614
Val:  [ 770/1033]  eta: 0:00:19  loss: 2.2643 (2.3850)  acc1: 50.0000 (45.2497)  acc5: 70.8333 (72.6708)  time: 0.0541 (0.0159 -- 0.4528)  data: 0.0216 (0.0001 -- 0.4199)  max mem: 1614
Val:  [ 780/1033]  eta: 0:00:18  loss: 2.3856 (2.3869)  acc1: 45.8333 (45.1985)  acc5: 70.8333 (72.6152)  time: 0.0555 (0.0193 -- 0.4740)  data: 0.0232 (0.0002 -- 0.4514)  max mem: 1614
Val:  [ 790/1033]  eta: 0:00:17  loss: 2.4221 (2.3862)  acc1: 45.8333 (45.2486)  acc5: 70.8333 (72.6243)  time: 0.0757 (0.0193 -- 0.4740)  data: 0.0453 (0.0002 -- 0.4514)  max mem: 1614
Val:  [ 800/1033]  eta: 0:00:17  loss: 2.1278 (2.3853)  acc1: 45.8333 (45.2559)  acc5: 75.0000 (72.6436)  time: 0.0674 (0.0205 -- 0.4621)  data: 0.0351 (0.0002 -- 0.4324)  max mem: 1614
Val:  [ 810/1033]  eta: 0:00:16  loss: 2.2998 (2.3846)  acc1: 45.8333 (45.2168)  acc5: 75.0000 (72.6880)  time: 0.0802 (0.0237 -- 0.7595)  data: 0.0493 (0.0002 -- 0.7265)  max mem: 1614
Val:  [ 820/1033]  eta: 0:00:15  loss: 2.4468 (2.3863)  acc1: 41.6667 (45.1685)  acc5: 75.0000 (72.6756)  time: 0.0844 (0.0237 -- 0.7595)  data: 0.0532 (0.0002 -- 0.7265)  max mem: 1614
Val:  [ 830/1033]  eta: 0:00:14  loss: 2.4611 (2.3848)  acc1: 41.6667 (45.1564)  acc5: 70.8333 (72.6735)  time: 0.0510 (0.0213 -- 0.3558)  data: 0.0172 (0.0002 -- 0.3252)  max mem: 1614
Val:  [ 840/1033]  eta: 0:00:14  loss: 2.1575 (2.3826)  acc1: 45.8333 (45.1893)  acc5: 70.8333 (72.7012)  time: 0.0593 (0.0213 -- 0.5466)  data: 0.0268 (0.0004 -- 0.5099)  max mem: 1614
Val:  [ 850/1033]  eta: 0:00:13  loss: 2.2199 (2.3801)  acc1: 45.8333 (45.1968)  acc5: 75.0000 (72.7331)  time: 0.0728 (0.0258 -- 0.5466)  data: 0.0394 (0.0003 -- 0.5099)  max mem: 1614
Val:  [ 860/1033]  eta: 0:00:12  loss: 2.3310 (2.3828)  acc1: 41.6667 (45.1558)  acc5: 70.8333 (72.6578)  time: 0.0772 (0.0248 -- 0.6290)  data: 0.0440 (0.0003 -- 0.5998)  max mem: 1614
Val:  [ 870/1033]  eta: 0:00:11  loss: 2.5746 (2.3859)  acc1: 41.6667 (45.1014)  acc5: 66.6667 (72.5890)  time: 0.0785 (0.0248 -- 0.6290)  data: 0.0454 (0.0003 -- 0.5998)  max mem: 1614
Val:  [ 880/1033]  eta: 0:00:11  loss: 2.3715 (2.3848)  acc1: 41.6667 (45.1286)  acc5: 70.8333 (72.6069)  time: 0.0669 (0.0201 -- 0.4330)  data: 0.0351 (0.0003 -- 0.4015)  max mem: 1614
Val:  [ 890/1033]  eta: 0:00:10  loss: 2.2430 (2.3853)  acc1: 45.8333 (45.1599)  acc5: 75.0000 (72.5823)  time: 0.0510 (0.0200 -- 0.4330)  data: 0.0216 (0.0002 -- 0.4015)  max mem: 1614
Val:  [ 900/1033]  eta: 0:00:09  loss: 2.1141 (2.3833)  acc1: 45.8333 (45.1813)  acc5: 75.0000 (72.6369)  time: 0.0550 (0.0187 -- 0.4768)  data: 0.0238 (0.0002 -- 0.4475)  max mem: 1614
Val:  [ 910/1033]  eta: 0:00:08  loss: 2.2797 (2.3842)  acc1: 41.6667 (45.1610)  acc5: 75.0000 (72.6262)  time: 0.0789 (0.0187 -- 0.5091)  data: 0.0472 (0.0003 -- 0.4846)  max mem: 1614
Val:  [ 920/1033]  eta: 0:00:08  loss: 2.3124 (2.3824)  acc1: 41.6667 (45.1412)  acc5: 75.0000 (72.6882)  time: 0.0832 (0.0246 -- 0.5510)  data: 0.0508 (0.0003 -- 0.5186)  max mem: 1614
Val:  [ 930/1033]  eta: 0:00:07  loss: 2.3124 (2.3825)  acc1: 37.5000 (45.1217)  acc5: 79.1667 (72.7041)  time: 0.0798 (0.0228 -- 0.5510)  data: 0.0465 (0.0002 -- 0.5186)  max mem: 1614
Val:  [ 940/1033]  eta: 0:00:06  loss: 2.3825 (2.3802)  acc1: 45.8333 (45.1736)  acc5: 75.0000 (72.7506)  time: 0.0724 (0.0228 -- 0.4312)  data: 0.0389 (0.0002 -- 0.3989)  max mem: 1614
Val:  [ 950/1033]  eta: 0:00:06  loss: 2.0639 (2.3802)  acc1: 50.0000 (45.1761)  acc5: 75.0000 (72.7962)  time: 0.0523 (0.0303 -- 0.3998)  data: 0.0192 (0.0003 -- 0.3680)  max mem: 1614
Val:  [ 960/1033]  eta: 0:00:05  loss: 2.1743 (2.3801)  acc1: 50.0000 (45.1743)  acc5: 75.0000 (72.7714)  time: 0.0473 (0.0276 -- 0.3209)  data: 0.0158 (0.0003 -- 0.2884)  max mem: 1614
Val:  [ 970/1033]  eta: 0:00:04  loss: 2.2922 (2.3808)  acc1: 50.0000 (45.1768)  acc5: 70.8333 (72.7386)  time: 0.0760 (0.0276 -- 0.6039)  data: 0.0444 (0.0003 -- 0.5702)  max mem: 1614
Val:  [ 980/1033]  eta: 0:00:03  loss: 2.4376 (2.3814)  acc1: 45.8333 (45.1877)  acc5: 70.8333 (72.7447)  time: 0.0882 (0.0293 -- 0.6039)  data: 0.0543 (0.0003 -- 0.5702)  max mem: 1614
Val:  [ 990/1033]  eta: 0:00:03  loss: 2.4476 (2.3818)  acc1: 41.6667 (45.1564)  acc5: 70.8333 (72.7716)  time: 0.0744 (0.0284 -- 0.5270)  data: 0.0404 (0.0002 -- 0.4815)  max mem: 1614
Val:  [1000/1033]  eta: 0:00:02  loss: 2.5132 (2.3830)  acc1: 41.6667 (45.1257)  acc5: 70.8333 (72.7481)  time: 0.0657 (0.0284 -- 0.3738)  data: 0.0325 (0.0002 -- 0.3384)  max mem: 1614
Val:  [1010/1033]  eta: 0:00:01  loss: 2.3970 (2.3821)  acc1: 41.6667 (45.1657)  acc5: 66.6667 (72.7291)  time: 0.0524 (0.0277 -- 0.3738)  data: 0.0186 (0.0001 -- 0.3384)  max mem: 1614
Val:  [1020/1033]  eta: 0:00:00  loss: 2.3283 (2.3846)  acc1: 41.6667 (45.1355)  acc5: 66.6667 (72.6616)  time: 0.0490 (0.0216 -- 0.3222)  data: 0.0162 (0.0001 -- 0.2953)  max mem: 1614
Val:  [1030/1033]  eta: 0:00:00  loss: 2.3477 (2.3834)  acc1: 45.8333 (45.1908)  acc5: 70.8333 (72.6681)  time: 0.0524 (0.0216 -- 0.3222)  data: 0.0223 (0.0001 -- 0.2953)  max mem: 1614
Val:  [1032/1033]  eta: 0:00:00  loss: 2.3283 (2.3822)  acc1: 45.8333 (45.2072)  acc5: 70.8333 (72.6642)  time: 0.0384 (0.0216 -- 0.1748)  data: 0.0076 (0.0001 -- 0.1470)  max mem: 1614
Val: Total time: 0:01:14 (0.0722 s / it)
* Acc@1 45.207 Acc@5 72.664 loss 2.382
24777 val images: Top-1 45.21%, Top-5 72.66%, loss 2.3822
