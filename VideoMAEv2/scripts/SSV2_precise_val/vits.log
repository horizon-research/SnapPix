+ unset SLURM_PROCID
+ export MASTER_PORT=27792
+ MASTER_PORT=27792
+ export OMP_NUM_THREADS=1
+ OMP_NUM_THREADS=1
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
+ OUTPUT_DIR=local/SSV2/vits
+ DATA_PATH=../dataset/shared_list/SSV2_Mine
+ python3 -m torch.distributed.launch --nproc_per_node=1 --master_port=27792 --nnodes=1 run_coded_class_finetuning.py --model coded_vit_small_patch8_112 --finetune local/coded_vits_pt_decor/checkpoint-299.pth --data_set SSV2 --nb_classes 174 --data_root /localdisk2/dataset/mmdataset --data_path ../dataset/shared_list/SSV2_Mine --log_dir local/SSV2/vits --output_dir local/SSV2/vits --batch_size 16 --num_sample 2 --input_size 112 --short_side_size 112 --save_ckpt_freq 20 --num_frames 16 --opt adamw --lr 2e-3 --num_workers 12 --opt_betas 0.9 0.999 --weight_decay 0.05 --layer_decay 0.65 --epochs 50 --test_num_segment 2 --test_num_crop 3 --local-rank 0 --update_freq 8 --warmup_epochs 8 --coded_template_folder ./decorrelation_training_wd0_norm_new --coded_type decor_fix --cross_model None --validation
/opt/conda/lib/python3.10/site-packages/torch/distributed/launch.py:183: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
[2024-11-17 05:30:24,815] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
| distributed init (rank 0): env://, gpu 0
Namespace(batch_size=16, epochs=50, update_freq=8, save_ckpt_freq=20, model='coded_vit_small_patch8_112', tubelet_size=1, input_size=112, with_checkpoint=False, drop=0.0, attn_drop_rate=0.0, drop_path=0.1, head_drop_rate=0.0, disable_eval_during_finetuning=False, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=[0.9, 0.999], clip_grad=None, momentum=0.9, weight_decay=0.05, weight_decay_end=None, lr=0.002, layer_decay=0.65, warmup_lr=1e-08, min_lr=1e-06, warmup_epochs=8, warmup_steps=-1, color_jitter=0.4, num_sample=2, aa='rand-m7-n4-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', crop_pct=None, short_side_size=112, test_num_segment=2, test_num_crop=3, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='local/coded_vits_pt_decor/checkpoint-299.pth', model_key='model|module', model_prefix='', init_scale=0.001, use_mean_pooling=True, data_path='../dataset/shared_list/SSV2_Mine', data_root='/localdisk2/dataset/mmdataset', eval_data_path=None, nb_classes=174, imagenet_default_mean_and_std=True, num_segments=1, num_frames=16, sampling_rate=4, sparse_sample=False, data_set='SSV2', fname_tmpl='img_{:05}.jpg', start_idx=1, output_dir='local/SSV2/vits', log_dir='local/SSV2/vits', device='cuda', seed=0, resume='', auto_resume=True, save_ckpt=True, start_epoch=0, eval=False, validation=True, dist_eval=False, num_workers=12, pin_mem=True, world_size=1, dist_on_itp=False, dist_url='env://', enable_deepspeed=False, debug=False, coded_type='decor_fix', coded_template_folder='./decorrelation_training_wd0_norm_new', local_rank=0, cross_model_path='None', rank=0, gpu=0, distributed=True, dist_backend='nccl')
Number of the class = 174
Number of the class = 174
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7ff22a38f760>
Mixup is activated!
loading pattern_path ./decorrelation_training_wd0_norm_new/pattern_4.pth
Patch size = (8, 8)
Load ckpt from local/coded_vits_pt_decor/checkpoint-299.pth
Load state_dict by model_key = model
Weights of CodedVisionTransformer not initialized from pretrained model: ['fc_norm.weight', 'fc_norm.bias', 'head.weight', 'head.bias']
Weights from pretrained model not used in CodedVisionTransformer: ['mask_token', 'decoder_pos_embed', 'decoder.blocks.0.norm1.weight', 'decoder.blocks.0.norm1.bias', 'decoder.blocks.0.attn.q_bias', 'decoder.blocks.0.attn.v_bias', 'decoder.blocks.0.attn.qkv.weight', 'decoder.blocks.0.attn.proj.weight', 'decoder.blocks.0.attn.proj.bias', 'decoder.blocks.0.norm2.weight', 'decoder.blocks.0.norm2.bias', 'decoder.blocks.0.mlp.fc1.weight', 'decoder.blocks.0.mlp.fc1.bias', 'decoder.blocks.0.mlp.fc2.weight', 'decoder.blocks.0.mlp.fc2.bias', 'decoder.blocks.1.norm1.weight', 'decoder.blocks.1.norm1.bias', 'decoder.blocks.1.attn.q_bias', 'decoder.blocks.1.attn.v_bias', 'decoder.blocks.1.attn.qkv.weight', 'decoder.blocks.1.attn.proj.weight', 'decoder.blocks.1.attn.proj.bias', 'decoder.blocks.1.norm2.weight', 'decoder.blocks.1.norm2.bias', 'decoder.blocks.1.mlp.fc1.weight', 'decoder.blocks.1.mlp.fc1.bias', 'decoder.blocks.1.mlp.fc2.weight', 'decoder.blocks.1.mlp.fc2.bias', 'decoder.blocks.2.norm1.weight', 'decoder.blocks.2.norm1.bias', 'decoder.blocks.2.attn.q_bias', 'decoder.blocks.2.attn.v_bias', 'decoder.blocks.2.attn.qkv.weight', 'decoder.blocks.2.attn.proj.weight', 'decoder.blocks.2.attn.proj.bias', 'decoder.blocks.2.norm2.weight', 'decoder.blocks.2.norm2.bias', 'decoder.blocks.2.mlp.fc1.weight', 'decoder.blocks.2.mlp.fc1.bias', 'decoder.blocks.2.mlp.fc2.weight', 'decoder.blocks.2.mlp.fc2.bias', 'decoder.blocks.3.norm1.weight', 'decoder.blocks.3.norm1.bias', 'decoder.blocks.3.attn.q_bias', 'decoder.blocks.3.attn.v_bias', 'decoder.blocks.3.attn.qkv.weight', 'decoder.blocks.3.attn.proj.weight', 'decoder.blocks.3.attn.proj.bias', 'decoder.blocks.3.norm2.weight', 'decoder.blocks.3.norm2.bias', 'decoder.blocks.3.mlp.fc1.weight', 'decoder.blocks.3.mlp.fc1.bias', 'decoder.blocks.3.mlp.fc2.weight', 'decoder.blocks.3.mlp.fc2.bias', 'decoder.norm.weight', 'decoder.norm.bias', 'decoder.head.weight', 'decoder.head.bias', 'encoder_to_decoder.weight', 'norm.weight', 'norm.bias']
Model = CodedVisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv3d(1, 384, kernel_size=(1, 8, 8), stride=(1, 8, 8))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.00909090880304575)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0181818176060915)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.027272727340459824)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.036363635212183)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.045454543083906174)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.054545458406209946)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.06363636255264282)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0727272778749466)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.08181818574666977)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.09090909361839294)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.10000000149011612)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): Identity()
  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (head_dropout): Dropout(p=0.0, inplace=False)
  (head): Linear(in_features=384, out_features=174, bias=True)
  (coded_layer): CodedLayer()
)
number of params: 21382702
LR = 0.00100000
Batch size = 128
Update frequent = 8
Number of training examples = 168913
Number of training training per epoch = 1319
Assigned values = [0.003697205891018715, 0.005688009063105715, 0.008750783174008792, 0.013462743344628911, 0.02071191283789063, 0.03186448128906251, 0.049022278906250015, 0.07541889062500001, 0.11602906250000002, 0.17850625000000003, 0.274625, 0.42250000000000004, 0.65, 1.0]
Skip weight decay list:  {'pos_embed', 'cls_token', 'coded_layer.coded_weight'}
Param groups = {
  "layer_0_decay": {
    "weight_decay": 0.05,
    "params": [
      "patch_embed.proj.weight"
    ],
    "lr_scale": 0.003697205891018715
  },
  "layer_0_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "patch_embed.proj.bias"
    ],
    "lr_scale": 0.003697205891018715
  },
  "layer_1_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.0.norm1.weight",
      "blocks.0.norm1.bias",
      "blocks.0.attn.q_bias",
      "blocks.0.attn.v_bias",
      "blocks.0.attn.proj.bias",
      "blocks.0.norm2.weight",
      "blocks.0.norm2.bias",
      "blocks.0.mlp.fc1.bias",
      "blocks.0.mlp.fc2.bias"
    ],
    "lr_scale": 0.005688009063105715
  },
  "layer_1_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.0.attn.qkv.weight",
      "blocks.0.attn.proj.weight",
      "blocks.0.mlp.fc1.weight",
      "blocks.0.mlp.fc2.weight"
    ],
    "lr_scale": 0.005688009063105715
  },
  "layer_2_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.1.norm1.weight",
      "blocks.1.norm1.bias",
      "blocks.1.attn.q_bias",
      "blocks.1.attn.v_bias",
      "blocks.1.attn.proj.bias",
      "blocks.1.norm2.weight",
      "blocks.1.norm2.bias",
      "blocks.1.mlp.fc1.bias",
      "blocks.1.mlp.fc2.bias"
    ],
    "lr_scale": 0.008750783174008792
  },
  "layer_2_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.1.attn.qkv.weight",
      "blocks.1.attn.proj.weight",
      "blocks.1.mlp.fc1.weight",
      "blocks.1.mlp.fc2.weight"
    ],
    "lr_scale": 0.008750783174008792
  },
  "layer_3_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.2.norm1.weight",
      "blocks.2.norm1.bias",
      "blocks.2.attn.q_bias",
      "blocks.2.attn.v_bias",
      "blocks.2.attn.proj.bias",
      "blocks.2.norm2.weight",
      "blocks.2.norm2.bias",
      "blocks.2.mlp.fc1.bias",
      "blocks.2.mlp.fc2.bias"
    ],
    "lr_scale": 0.013462743344628911
  },
  "layer_3_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.2.attn.qkv.weight",
      "blocks.2.attn.proj.weight",
      "blocks.2.mlp.fc1.weight",
      "blocks.2.mlp.fc2.weight"
    ],
    "lr_scale": 0.013462743344628911
  },
  "layer_4_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.3.norm1.weight",
      "blocks.3.norm1.bias",
      "blocks.3.attn.q_bias",
      "blocks.3.attn.v_bias",
      "blocks.3.attn.proj.bias",
      "blocks.3.norm2.weight",
      "blocks.3.norm2.bias",
      "blocks.3.mlp.fc1.bias",
      "blocks.3.mlp.fc2.bias"
    ],
    "lr_scale": 0.02071191283789063
  },
  "layer_4_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.3.attn.qkv.weight",
      "blocks.3.attn.proj.weight",
      "blocks.3.mlp.fc1.weight",
      "blocks.3.mlp.fc2.weight"
    ],
    "lr_scale": 0.02071191283789063
  },
  "layer_5_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.4.norm1.weight",
      "blocks.4.norm1.bias",
      "blocks.4.attn.q_bias",
      "blocks.4.attn.v_bias",
      "blocks.4.attn.proj.bias",
      "blocks.4.norm2.weight",
      "blocks.4.norm2.bias",
      "blocks.4.mlp.fc1.bias",
      "blocks.4.mlp.fc2.bias"
    ],
    "lr_scale": 0.03186448128906251
  },
  "layer_5_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.4.attn.qkv.weight",
      "blocks.4.attn.proj.weight",
      "blocks.4.mlp.fc1.weight",
      "blocks.4.mlp.fc2.weight"
    ],
    "lr_scale": 0.03186448128906251
  },
  "layer_6_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.5.norm1.weight",
      "blocks.5.norm1.bias",
      "blocks.5.attn.q_bias",
      "blocks.5.attn.v_bias",
      "blocks.5.attn.proj.bias",
      "blocks.5.norm2.weight",
      "blocks.5.norm2.bias",
      "blocks.5.mlp.fc1.bias",
      "blocks.5.mlp.fc2.bias"
    ],
    "lr_scale": 0.049022278906250015
  },
  "layer_6_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.5.attn.qkv.weight",
      "blocks.5.attn.proj.weight",
      "blocks.5.mlp.fc1.weight",
      "blocks.5.mlp.fc2.weight"
    ],
    "lr_scale": 0.049022278906250015
  },
  "layer_7_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.6.norm1.weight",
      "blocks.6.norm1.bias",
      "blocks.6.attn.q_bias",
      "blocks.6.attn.v_bias",
      "blocks.6.attn.proj.bias",
      "blocks.6.norm2.weight",
      "blocks.6.norm2.bias",
      "blocks.6.mlp.fc1.bias",
      "blocks.6.mlp.fc2.bias"
    ],
    "lr_scale": 0.07541889062500001
  },
  "layer_7_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.6.attn.qkv.weight",
      "blocks.6.attn.proj.weight",
      "blocks.6.mlp.fc1.weight",
      "blocks.6.mlp.fc2.weight"
    ],
    "lr_scale": 0.07541889062500001
  },
  "layer_8_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.7.norm1.weight",
      "blocks.7.norm1.bias",
      "blocks.7.attn.q_bias",
      "blocks.7.attn.v_bias",
      "blocks.7.attn.proj.bias",
      "blocks.7.norm2.weight",
      "blocks.7.norm2.bias",
      "blocks.7.mlp.fc1.bias",
      "blocks.7.mlp.fc2.bias"
    ],
    "lr_scale": 0.11602906250000002
  },
  "layer_8_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.7.attn.qkv.weight",
      "blocks.7.attn.proj.weight",
      "blocks.7.mlp.fc1.weight",
      "blocks.7.mlp.fc2.weight"
    ],
    "lr_scale": 0.11602906250000002
  },
  "layer_9_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.8.norm1.weight",
      "blocks.8.norm1.bias",
      "blocks.8.attn.q_bias",
      "blocks.8.attn.v_bias",
      "blocks.8.attn.proj.bias",
      "blocks.8.norm2.weight",
      "blocks.8.norm2.bias",
      "blocks.8.mlp.fc1.bias",
      "blocks.8.mlp.fc2.bias"
    ],
    "lr_scale": 0.17850625000000003
  },
  "layer_9_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.8.attn.qkv.weight",
      "blocks.8.attn.proj.weight",
      "blocks.8.mlp.fc1.weight",
      "blocks.8.mlp.fc2.weight"
    ],
    "lr_scale": 0.17850625000000003
  },
  "layer_10_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.9.norm1.weight",
      "blocks.9.norm1.bias",
      "blocks.9.attn.q_bias",
      "blocks.9.attn.v_bias",
      "blocks.9.attn.proj.bias",
      "blocks.9.norm2.weight",
      "blocks.9.norm2.bias",
      "blocks.9.mlp.fc1.bias",
      "blocks.9.mlp.fc2.bias"
    ],
    "lr_scale": 0.274625
  },
  "layer_10_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.9.attn.qkv.weight",
      "blocks.9.attn.proj.weight",
      "blocks.9.mlp.fc1.weight",
      "blocks.9.mlp.fc2.weight"
    ],
    "lr_scale": 0.274625
  },
  "layer_11_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.10.norm1.weight",
      "blocks.10.norm1.bias",
      "blocks.10.attn.q_bias",
      "blocks.10.attn.v_bias",
      "blocks.10.attn.proj.bias",
      "blocks.10.norm2.weight",
      "blocks.10.norm2.bias",
      "blocks.10.mlp.fc1.bias",
      "blocks.10.mlp.fc2.bias"
    ],
    "lr_scale": 0.42250000000000004
  },
  "layer_11_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.10.attn.qkv.weight",
      "blocks.10.attn.proj.weight",
      "blocks.10.mlp.fc1.weight",
      "blocks.10.mlp.fc2.weight"
    ],
    "lr_scale": 0.42250000000000004
  },
  "layer_12_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.11.norm1.weight",
      "blocks.11.norm1.bias",
      "blocks.11.attn.q_bias",
      "blocks.11.attn.v_bias",
      "blocks.11.attn.proj.bias",
      "blocks.11.norm2.weight",
      "blocks.11.norm2.bias",
      "blocks.11.mlp.fc1.bias",
      "blocks.11.mlp.fc2.bias"
    ],
    "lr_scale": 0.65
  },
  "layer_12_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.11.attn.qkv.weight",
      "blocks.11.attn.proj.weight",
      "blocks.11.mlp.fc1.weight",
      "blocks.11.mlp.fc2.weight"
    ],
    "lr_scale": 0.65
  },
  "layer_13_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "fc_norm.weight",
      "fc_norm.bias",
      "head.bias",
      "coded_layer.coded_weight"
    ],
    "lr_scale": 1.0
  },
  "layer_13_decay": {
    "weight_decay": 0.05,
    "params": [
      "head.weight"
    ],
    "lr_scale": 1.0
  }
}
optimizer settings: {'lr': 0.001, 'weight_decay': 0.0, 'eps': 1e-08, 'betas': [0.9, 0.999]}
Use step level LR scheduler!
Set warmup steps = 10552
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
criterion = SoftTargetCrossEntropy()
Auto resume checkpoint: local/SSV2/vits/checkpoint-49.pth
Resume checkpoint local/SSV2/vits/checkpoint-49.pth
With optim & sched!
Val:  [   0/1033]  eta: 0:57:19  loss: 2.5140 (2.5140)  acc1: 41.6667 (41.6667)  acc5: 70.8333 (70.8333)  time: 3.3297 (3.3297 -- 3.3297)  data: 1.7735 (1.7735 -- 1.7735)  max mem: 461
Val:  [  10/1033]  eta: 0:05:18  loss: 2.4500 (2.3360)  acc1: 41.6667 (45.4545)  acc5: 70.8333 (74.2424)  time: 0.3111 (0.0067 -- 3.3297)  data: 0.1615 (0.0001 -- 1.7735)  max mem: 461
Val:  [  20/1033]  eta: 0:02:50  loss: 2.3885 (2.4272)  acc1: 41.6667 (42.8571)  acc5: 70.8333 (72.8175)  time: 0.0101 (0.0067 -- 0.0169)  data: 0.0003 (0.0001 -- 0.0009)  max mem: 461
Val:  [  30/1033]  eta: 0:02:23  loss: 2.4781 (2.5040)  acc1: 37.5000 (40.0538)  acc5: 66.6667 (70.4301)  time: 0.0499 (0.0085 -- 0.6601)  data: 0.0367 (0.0001 -- 0.6382)  max mem: 461
Val:  [  40/1033]  eta: 0:02:07  loss: 2.5983 (2.5097)  acc1: 33.3333 (39.8374)  acc5: 66.6667 (70.0203)  time: 0.0869 (0.0097 -- 0.6601)  data: 0.0718 (0.0002 -- 0.6382)  max mem: 461
Val:  [  50/1033]  eta: 0:02:01  loss: 2.5253 (2.4876)  acc1: 41.6667 (40.9314)  acc5: 66.6667 (70.2614)  time: 0.0945 (0.0077 -- 0.5581)  data: 0.0821 (0.0001 -- 0.5401)  max mem: 461
Val:  [  60/1033]  eta: 0:01:42  loss: 2.5253 (2.5038)  acc1: 41.6667 (40.5055)  acc5: 66.6667 (69.8771)  time: 0.0589 (0.0077 -- 0.5581)  data: 0.0486 (0.0001 -- 0.5401)  max mem: 461
Val:  [  70/1033]  eta: 0:01:40  loss: 2.3155 (2.4616)  acc1: 41.6667 (41.6667)  acc5: 70.8333 (70.5986)  time: 0.0564 (0.0088 -- 0.4050)  data: 0.0445 (0.0001 -- 0.3941)  max mem: 461
Val:  [  80/1033]  eta: 0:01:37  loss: 2.2818 (2.4578)  acc1: 45.8333 (41.8724)  acc5: 75.0000 (70.8333)  time: 0.0935 (0.0081 -- 0.4050)  data: 0.0807 (0.0001 -- 0.3941)  max mem: 461
Val:  [  90/1033]  eta: 0:01:35  loss: 2.4255 (2.4586)  acc1: 41.6667 (41.9872)  acc5: 70.8333 (70.9249)  time: 0.0870 (0.0081 -- 0.3243)  data: 0.0725 (0.0001 -- 0.3094)  max mem: 461
Val:  [ 100/1033]  eta: 0:01:29  loss: 2.4503 (2.4648)  acc1: 41.6667 (41.9142)  acc5: 70.8333 (70.7921)  time: 0.0676 (0.0101 -- 0.3243)  data: 0.0491 (0.0001 -- 0.3094)  max mem: 461
Val:  [ 110/1033]  eta: 0:01:26  loss: 2.3193 (2.4417)  acc1: 45.8333 (42.3799)  acc5: 70.8333 (71.2462)  time: 0.0620 (0.0140 -- 0.4443)  data: 0.0435 (0.0003 -- 0.4281)  max mem: 461
Val:  [ 120/1033]  eta: 0:01:23  loss: 2.3426 (2.4572)  acc1: 45.8333 (41.9077)  acc5: 70.8333 (70.9711)  time: 0.0704 (0.0103 -- 0.4955)  data: 0.0530 (0.0001 -- 0.4770)  max mem: 461
Val:  [ 130/1033]  eta: 0:01:22  loss: 2.3933 (2.4436)  acc1: 41.6667 (41.9847)  acc5: 70.8333 (71.4377)  time: 0.0780 (0.0103 -- 0.4955)  data: 0.0591 (0.0001 -- 0.4770)  max mem: 461
Val:  [ 140/1033]  eta: 0:01:19  loss: 2.3922 (2.4515)  acc1: 41.6667 (41.9031)  acc5: 70.8333 (71.1584)  time: 0.0775 (0.0098 -- 0.4835)  data: 0.0598 (0.0001 -- 0.4554)  max mem: 461
Val:  [ 150/1033]  eta: 0:01:19  loss: 2.5599 (2.4589)  acc1: 37.5000 (41.7494)  acc5: 66.6667 (70.9989)  time: 0.0785 (0.0098 -- 0.5110)  data: 0.0616 (0.0001 -- 0.4952)  max mem: 461
Val:  [ 160/1033]  eta: 0:01:15  loss: 2.5749 (2.4651)  acc1: 41.6667 (41.8478)  acc5: 66.6667 (70.7557)  time: 0.0606 (0.0120 -- 0.5110)  data: 0.0406 (0.0002 -- 0.4952)  max mem: 461
Val:  [ 170/1033]  eta: 0:01:13  loss: 2.5382 (2.4635)  acc1: 41.6667 (41.8372)  acc5: 66.6667 (70.9064)  time: 0.0483 (0.0122 -- 0.5226)  data: 0.0291 (0.0001 -- 0.5031)  max mem: 461
Val:  [ 180/1033]  eta: 0:01:12  loss: 2.5750 (2.4652)  acc1: 37.5000 (41.8969)  acc5: 70.8333 (70.8794)  time: 0.0755 (0.0122 -- 0.6331)  data: 0.0568 (0.0001 -- 0.6190)  max mem: 461
Val:  [ 190/1033]  eta: 0:01:11  loss: 2.6046 (2.4698)  acc1: 37.5000 (41.8412)  acc5: 70.8333 (70.8552)  time: 0.0798 (0.0128 -- 0.6331)  data: 0.0591 (0.0002 -- 0.6190)  max mem: 461
Val:  [ 200/1033]  eta: 0:01:10  loss: 2.3286 (2.4630)  acc1: 41.6667 (42.1227)  acc5: 70.8333 (71.0199)  time: 0.0814 (0.0128 -- 0.5164)  data: 0.0614 (0.0001 -- 0.4891)  max mem: 461
Val:  [ 210/1033]  eta: 0:01:09  loss: 2.4123 (2.4580)  acc1: 45.8333 (42.2788)  acc5: 70.8333 (71.0703)  time: 0.0847 (0.0132 -- 0.5742)  data: 0.0623 (0.0001 -- 0.5623)  max mem: 461
Val:  [ 220/1033]  eta: 0:01:06  loss: 2.4123 (2.4547)  acc1: 41.6667 (42.3831)  acc5: 70.8333 (71.1161)  time: 0.0530 (0.0092 -- 0.5742)  data: 0.0328 (0.0001 -- 0.5623)  max mem: 461
Val:  [ 230/1033]  eta: 0:01:05  loss: 2.4165 (2.4546)  acc1: 37.5000 (42.3701)  acc5: 75.0000 (71.1580)  time: 0.0522 (0.0092 -- 0.6577)  data: 0.0365 (0.0001 -- 0.6447)  max mem: 461
Val:  [ 240/1033]  eta: 0:01:04  loss: 2.3166 (2.4472)  acc1: 45.8333 (42.5657)  acc5: 75.0000 (71.2656)  time: 0.0809 (0.0099 -- 0.6577)  data: 0.0651 (0.0001 -- 0.6447)  max mem: 461
Val:  [ 250/1033]  eta: 0:01:03  loss: 2.2101 (2.4453)  acc1: 45.8333 (42.5299)  acc5: 75.0000 (71.2981)  time: 0.0818 (0.0099 -- 0.6857)  data: 0.0663 (0.0001 -- 0.6689)  max mem: 461
Val:  [ 260/1033]  eta: 0:01:02  loss: 2.4886 (2.4495)  acc1: 37.5000 (42.4330)  acc5: 75.0000 (71.2484)  time: 0.0750 (0.0099 -- 0.6857)  data: 0.0593 (0.0001 -- 0.6689)  max mem: 461
Val:  [ 270/1033]  eta: 0:01:01  loss: 2.5484 (2.4549)  acc1: 37.5000 (42.2970)  acc5: 70.8333 (71.1716)  time: 0.0762 (0.0123 -- 0.4320)  data: 0.0613 (0.0001 -- 0.4220)  max mem: 461
Val:  [ 280/1033]  eta: 0:01:00  loss: 2.7246 (2.4631)  acc1: 33.3333 (42.0967)  acc5: 66.6667 (70.9816)  time: 0.0647 (0.0125 -- 0.4320)  data: 0.0478 (0.0003 -- 0.4220)  max mem: 461
Val:  [ 290/1033]  eta: 0:00:58  loss: 2.6910 (2.4624)  acc1: 37.5000 (42.1678)  acc5: 66.6667 (71.0052)  time: 0.0508 (0.0125 -- 0.3839)  data: 0.0314 (0.0002 -- 0.3659)  max mem: 461
Val:  [ 300/1033]  eta: 0:00:57  loss: 2.2307 (2.4610)  acc1: 41.6667 (42.1512)  acc5: 75.0000 (71.0687)  time: 0.0640 (0.0103 -- 0.3839)  data: 0.0443 (0.0002 -- 0.3659)  max mem: 461
Val:  [ 310/1033]  eta: 0:00:56  loss: 2.5264 (2.4673)  acc1: 37.5000 (42.0016)  acc5: 66.6667 (70.9003)  time: 0.0713 (0.0103 -- 0.4299)  data: 0.0538 (0.0001 -- 0.4125)  max mem: 461
Val:  [ 320/1033]  eta: 0:00:56  loss: 2.5264 (2.4664)  acc1: 37.5000 (41.9912)  acc5: 66.6667 (70.9631)  time: 0.0751 (0.0113 -- 0.4299)  data: 0.0583 (0.0001 -- 0.4125)  max mem: 461
Val:  [ 330/1033]  eta: 0:00:55  loss: 2.5750 (2.4743)  acc1: 37.5000 (41.8429)  acc5: 70.8333 (70.8837)  time: 0.0760 (0.0113 -- 0.3834)  data: 0.0586 (0.0001 -- 0.3682)  max mem: 461
Val:  [ 340/1033]  eta: 0:00:53  loss: 2.5750 (2.4739)  acc1: 37.5000 (41.9110)  acc5: 70.8333 (70.8456)  time: 0.0661 (0.0112 -- 0.4186)  data: 0.0491 (0.0001 -- 0.4006)  max mem: 461
Val:  [ 350/1033]  eta: 0:00:52  loss: 2.3442 (2.4688)  acc1: 45.8333 (42.0584)  acc5: 70.8333 (70.8927)  time: 0.0463 (0.0077 -- 0.4186)  data: 0.0289 (0.0001 -- 0.4006)  max mem: 461
Val:  [ 360/1033]  eta: 0:00:51  loss: 2.1078 (2.4625)  acc1: 45.8333 (42.2669)  acc5: 75.0000 (71.0065)  time: 0.0504 (0.0077 -- 0.4906)  data: 0.0344 (0.0001 -- 0.4793)  max mem: 461
Val:  [ 370/1033]  eta: 0:00:50  loss: 2.4459 (2.4669)  acc1: 45.8333 (42.1608)  acc5: 70.8333 (70.9232)  time: 0.0783 (0.0087 -- 0.7436)  data: 0.0622 (0.0002 -- 0.7234)  max mem: 461
Val:  [ 380/1033]  eta: 0:00:50  loss: 2.4744 (2.4655)  acc1: 37.5000 (42.1807)  acc5: 70.8333 (70.9536)  time: 0.0855 (0.0124 -- 0.7436)  data: 0.0667 (0.0002 -- 0.7234)  max mem: 461
Val:  [ 390/1033]  eta: 0:00:49  loss: 2.5001 (2.4668)  acc1: 41.6667 (42.2315)  acc5: 70.8333 (70.8866)  time: 0.0822 (0.0112 -- 0.6955)  data: 0.0646 (0.0001 -- 0.6778)  max mem: 461
Val:  [ 400/1033]  eta: 0:00:48  loss: 2.2520 (2.4620)  acc1: 45.8333 (42.3525)  acc5: 70.8333 (70.9892)  time: 0.0793 (0.0112 -- 0.6955)  data: 0.0635 (0.0001 -- 0.6778)  max mem: 461
Val:  [ 410/1033]  eta: 0:00:47  loss: 2.1280 (2.4565)  acc1: 50.0000 (42.4676)  acc5: 75.0000 (71.1071)  time: 0.0454 (0.0114 -- 0.5930)  data: 0.0296 (0.0002 -- 0.5772)  max mem: 461
Val:  [ 420/1033]  eta: 0:00:46  loss: 2.3163 (2.4504)  acc1: 50.0000 (42.6960)  acc5: 70.8333 (71.1797)  time: 0.0432 (0.0121 -- 0.5312)  data: 0.0271 (0.0002 -- 0.5128)  max mem: 461
Val:  [ 430/1033]  eta: 0:00:45  loss: 2.4167 (2.4526)  acc1: 45.8333 (42.7301)  acc5: 70.8333 (71.1137)  time: 0.0776 (0.0139 -- 0.5953)  data: 0.0603 (0.0002 -- 0.5778)  max mem: 461
Val:  [ 440/1033]  eta: 0:00:45  loss: 2.4831 (2.4541)  acc1: 37.5000 (42.6493)  acc5: 66.6667 (71.0412)  time: 0.0895 (0.0139 -- 0.5953)  data: 0.0693 (0.0002 -- 0.5778)  max mem: 461
Val:  [ 450/1033]  eta: 0:00:44  loss: 2.4187 (2.4517)  acc1: 37.5000 (42.7384)  acc5: 70.8333 (71.1013)  time: 0.0800 (0.0140 -- 0.4979)  data: 0.0582 (0.0002 -- 0.4821)  max mem: 461
Val:  [ 460/1033]  eta: 0:00:43  loss: 2.3564 (2.4523)  acc1: 45.8333 (42.7151)  acc5: 70.8333 (71.0954)  time: 0.0610 (0.0119 -- 0.4126)  data: 0.0427 (0.0001 -- 0.3963)  max mem: 461
Val:  [ 470/1033]  eta: 0:00:41  loss: 2.3564 (2.4507)  acc1: 37.5000 (42.7017)  acc5: 66.6667 (71.1341)  time: 0.0468 (0.0119 -- 0.4126)  data: 0.0300 (0.0001 -- 0.3963)  max mem: 461
Val:  [ 480/1033]  eta: 0:00:41  loss: 2.3411 (2.4479)  acc1: 41.6667 (42.7755)  acc5: 70.8333 (71.1798)  time: 0.0597 (0.0124 -- 0.3233)  data: 0.0407 (0.0002 -- 0.3076)  max mem: 461
Val:  [ 490/1033]  eta: 0:00:40  loss: 2.4129 (2.4491)  acc1: 45.8333 (42.8208)  acc5: 70.8333 (71.1049)  time: 0.0889 (0.0144 -- 0.4948)  data: 0.0692 (0.0002 -- 0.4799)  max mem: 461
Val:  [ 500/1033]  eta: 0:00:40  loss: 2.6205 (2.4510)  acc1: 41.6667 (42.8310)  acc5: 66.6667 (71.0246)  time: 0.0902 (0.0121 -- 0.6573)  data: 0.0700 (0.0002 -- 0.6287)  max mem: 461
Val:  [ 510/1033]  eta: 0:00:39  loss: 2.4665 (2.4491)  acc1: 37.5000 (42.8327)  acc5: 70.8333 (71.0780)  time: 0.0800 (0.0121 -- 0.6573)  data: 0.0578 (0.0001 -- 0.6287)  max mem: 461
Val:  [ 520/1033]  eta: 0:00:38  loss: 2.4665 (2.4502)  acc1: 37.5000 (42.8023)  acc5: 70.8333 (71.0333)  time: 0.0479 (0.0140 -- 0.5345)  data: 0.0265 (0.0001 -- 0.5186)  max mem: 461
Val:  [ 530/1033]  eta: 0:00:37  loss: 2.4248 (2.4483)  acc1: 41.6667 (42.8202)  acc5: 70.8333 (71.0923)  time: 0.0439 (0.0139 -- 0.5089)  data: 0.0252 (0.0001 -- 0.4934)  max mem: 461
Val:  [ 540/1033]  eta: 0:00:36  loss: 2.3955 (2.4516)  acc1: 41.6667 (42.7757)  acc5: 70.8333 (70.9643)  time: 0.0673 (0.0106 -- 0.5089)  data: 0.0496 (0.0001 -- 0.4934)  max mem: 461
Val:  [ 550/1033]  eta: 0:00:35  loss: 2.3618 (2.4494)  acc1: 41.6667 (42.7480)  acc5: 70.8333 (71.0299)  time: 0.0627 (0.0106 -- 0.5071)  data: 0.0438 (0.0002 -- 0.4809)  max mem: 461
Val:  [ 560/1033]  eta: 0:00:34  loss: 2.3618 (2.4504)  acc1: 37.5000 (42.7510)  acc5: 70.8333 (71.0116)  time: 0.0721 (0.0097 -- 0.6210)  data: 0.0548 (0.0002 -- 0.6040)  max mem: 461
Val:  [ 570/1033]  eta: 0:00:34  loss: 2.4155 (2.4501)  acc1: 37.5000 (42.7539)  acc5: 70.8333 (71.0012)  time: 0.0780 (0.0087 -- 0.6210)  data: 0.0639 (0.0002 -- 0.6040)  max mem: 461
Val:  [ 580/1033]  eta: 0:00:33  loss: 2.3788 (2.4507)  acc1: 41.6667 (42.7711)  acc5: 70.8333 (70.9839)  time: 0.0440 (0.0087 -- 0.5514)  data: 0.0286 (0.0002 -- 0.5329)  max mem: 461
Val:  [ 590/1033]  eta: 0:00:32  loss: 2.5294 (2.4543)  acc1: 41.6667 (42.7101)  acc5: 70.8333 (70.9320)  time: 0.0526 (0.0102 -- 0.7385)  data: 0.0358 (0.0002 -- 0.7040)  max mem: 461
Val:  [ 600/1033]  eta: 0:00:31  loss: 2.5795 (2.4547)  acc1: 41.6667 (42.6581)  acc5: 70.8333 (70.9512)  time: 0.0852 (0.0101 -- 0.7385)  data: 0.0687 (0.0002 -- 0.7040)  max mem: 461
Val:  [ 610/1033]  eta: 0:00:31  loss: 2.5795 (2.4567)  acc1: 41.6667 (42.5805)  acc5: 70.8333 (70.9356)  time: 0.0854 (0.0101 -- 0.7265)  data: 0.0680 (0.0002 -- 0.6896)  max mem: 461
Val:  [ 620/1033]  eta: 0:00:30  loss: 2.3806 (2.4584)  acc1: 37.5000 (42.5456)  acc5: 70.8333 (70.9139)  time: 0.0822 (0.0118 -- 0.7265)  data: 0.0656 (0.0001 -- 0.6896)  max mem: 461
Val:  [ 630/1033]  eta: 0:00:29  loss: 2.4420 (2.4582)  acc1: 41.6667 (42.5515)  acc5: 70.8333 (70.8994)  time: 0.0795 (0.0118 -- 0.6565)  data: 0.0630 (0.0001 -- 0.6373)  max mem: 461
Val:  [ 640/1033]  eta: 0:00:28  loss: 2.4795 (2.4592)  acc1: 41.6667 (42.5247)  acc5: 66.6667 (70.8658)  time: 0.0512 (0.0141 -- 0.6565)  data: 0.0327 (0.0002 -- 0.6373)  max mem: 461
Val:  [ 650/1033]  eta: 0:00:28  loss: 2.5022 (2.4603)  acc1: 37.5000 (42.4859)  acc5: 66.6667 (70.8461)  time: 0.0560 (0.0097 -- 0.7611)  data: 0.0383 (0.0001 -- 0.7470)  max mem: 461
Val:  [ 660/1033]  eta: 0:00:27  loss: 2.5022 (2.4629)  acc1: 37.5000 (42.3853)  acc5: 66.6667 (70.8207)  time: 0.0859 (0.0097 -- 0.7611)  data: 0.0689 (0.0001 -- 0.7470)  max mem: 461
Val:  [ 670/1033]  eta: 0:00:26  loss: 2.4206 (2.4617)  acc1: 37.5000 (42.4305)  acc5: 70.8333 (70.8458)  time: 0.0776 (0.0101 -- 0.6412)  data: 0.0590 (0.0001 -- 0.6201)  max mem: 461
Val:  [ 680/1033]  eta: 0:00:25  loss: 2.4206 (2.4631)  acc1: 41.6667 (42.4131)  acc5: 70.8333 (70.8272)  time: 0.0659 (0.0101 -- 0.5682)  data: 0.0473 (0.0001 -- 0.5502)  max mem: 461
Val:  [ 690/1033]  eta: 0:00:25  loss: 2.6186 (2.4652)  acc1: 37.5000 (42.3722)  acc5: 62.5000 (70.7610)  time: 0.0674 (0.0112 -- 0.5767)  data: 0.0482 (0.0003 -- 0.5567)  max mem: 461
Val:  [ 700/1033]  eta: 0:00:24  loss: 2.5473 (2.4655)  acc1: 37.5000 (42.3978)  acc5: 66.6667 (70.7620)  time: 0.0478 (0.0125 -- 0.5767)  data: 0.0290 (0.0002 -- 0.5567)  max mem: 461
Val:  [ 710/1033]  eta: 0:00:23  loss: 2.4976 (2.4665)  acc1: 41.6667 (42.3582)  acc5: 66.6667 (70.7044)  time: 0.0563 (0.0125 -- 0.7237)  data: 0.0359 (0.0002 -- 0.7007)  max mem: 461
Val:  [ 720/1033]  eta: 0:00:22  loss: 2.5351 (2.4686)  acc1: 37.5000 (42.3081)  acc5: 66.6667 (70.6889)  time: 0.0854 (0.0157 -- 0.7237)  data: 0.0627 (0.0001 -- 0.7007)  max mem: 461
Val:  [ 730/1033]  eta: 0:00:22  loss: 2.5549 (2.4697)  acc1: 37.5000 (42.3450)  acc5: 70.8333 (70.6737)  time: 0.0752 (0.0108 -- 0.5876)  data: 0.0559 (0.0001 -- 0.5728)  max mem: 461
Val:  [ 740/1033]  eta: 0:00:21  loss: 2.5549 (2.4732)  acc1: 37.5000 (42.2515)  acc5: 66.6667 (70.5859)  time: 0.0827 (0.0108 -- 0.7473)  data: 0.0657 (0.0001 -- 0.7347)  max mem: 461
Val:  [ 750/1033]  eta: 0:00:20  loss: 2.6423 (2.4755)  acc1: 33.3333 (42.2159)  acc5: 62.5000 (70.5282)  time: 0.0799 (0.0129 -- 0.7473)  data: 0.0615 (0.0001 -- 0.7347)  max mem: 461
Val:  [ 760/1033]  eta: 0:00:19  loss: 2.6389 (2.4755)  acc1: 41.6667 (42.2361)  acc5: 66.6667 (70.5048)  time: 0.0440 (0.0141 -- 0.4905)  data: 0.0249 (0.0002 -- 0.4726)  max mem: 461
Val:  [ 770/1033]  eta: 0:00:18  loss: 2.5051 (2.4749)  acc1: 45.8333 (42.2882)  acc5: 66.6667 (70.4929)  time: 0.0459 (0.0139 -- 0.5917)  data: 0.0295 (0.0002 -- 0.5772)  max mem: 461
Val:  [ 780/1033]  eta: 0:00:18  loss: 2.5051 (2.4762)  acc1: 45.8333 (42.2695)  acc5: 66.6667 (70.4439)  time: 0.0740 (0.0094 -- 0.6399)  data: 0.0607 (0.0001 -- 0.6271)  max mem: 461
Val:  [ 790/1033]  eta: 0:00:17  loss: 2.4688 (2.4748)  acc1: 41.6667 (42.3198)  acc5: 66.6667 (70.4330)  time: 0.0799 (0.0094 -- 0.7113)  data: 0.0664 (0.0001 -- 0.6962)  max mem: 461
Val:  [ 800/1033]  eta: 0:00:16  loss: 2.3597 (2.4740)  acc1: 41.6667 (42.3273)  acc5: 75.0000 (70.4692)  time: 0.0825 (0.0096 -- 0.7113)  data: 0.0672 (0.0001 -- 0.6962)  max mem: 461
Val:  [ 810/1033]  eta: 0:00:16  loss: 2.3130 (2.4737)  acc1: 41.6667 (42.3346)  acc5: 75.0000 (70.4994)  time: 0.0765 (0.0096 -- 0.6564)  data: 0.0614 (0.0002 -- 0.6435)  max mem: 461
Val:  [ 820/1033]  eta: 0:00:15  loss: 2.4652 (2.4751)  acc1: 41.6667 (42.3061)  acc5: 70.8333 (70.4933)  time: 0.0452 (0.0096 -- 0.5865)  data: 0.0293 (0.0002 -- 0.5716)  max mem: 461
Val:  [ 830/1033]  eta: 0:00:14  loss: 2.5132 (2.4738)  acc1: 41.6667 (42.3085)  acc5: 70.8333 (70.5074)  time: 0.0498 (0.0086 -- 0.6790)  data: 0.0337 (0.0001 -- 0.6664)  max mem: 461
Val:  [ 840/1033]  eta: 0:00:13  loss: 2.4377 (2.4729)  acc1: 41.6667 (42.3405)  acc5: 70.8333 (70.5410)  time: 0.0887 (0.0086 -- 0.8334)  data: 0.0746 (0.0001 -- 0.8182)  max mem: 461
Val:  [ 850/1033]  eta: 0:00:13  loss: 2.4001 (2.4707)  acc1: 41.6667 (42.3766)  acc5: 70.8333 (70.5787)  time: 0.0885 (0.0096 -- 0.8334)  data: 0.0738 (0.0001 -- 0.8182)  max mem: 461
Val:  [ 860/1033]  eta: 0:00:12  loss: 2.4666 (2.4735)  acc1: 41.6667 (42.3587)  acc5: 66.6667 (70.5236)  time: 0.0820 (0.0116 -- 0.6688)  data: 0.0649 (0.0001 -- 0.6420)  max mem: 461
Val:  [ 870/1033]  eta: 0:00:11  loss: 2.5726 (2.4751)  acc1: 33.3333 (42.3173)  acc5: 66.6667 (70.5128)  time: 0.0739 (0.0101 -- 0.6688)  data: 0.0547 (0.0001 -- 0.6420)  max mem: 461
Val:  [ 880/1033]  eta: 0:00:10  loss: 2.4732 (2.4743)  acc1: 41.6667 (42.3241)  acc5: 70.8333 (70.5023)  time: 0.0416 (0.0101 -- 0.4548)  data: 0.0228 (0.0003 -- 0.4400)  max mem: 461
Val:  [ 890/1033]  eta: 0:00:10  loss: 2.4201 (2.4734)  acc1: 41.6667 (42.3494)  acc5: 70.8333 (70.5107)  time: 0.0511 (0.0124 -- 0.6638)  data: 0.0324 (0.0002 -- 0.6379)  max mem: 461
Val:  [ 900/1033]  eta: 0:00:09  loss: 2.2090 (2.4716)  acc1: 45.8333 (42.3973)  acc5: 75.0000 (70.5512)  time: 0.0675 (0.0139 -- 0.6638)  data: 0.0486 (0.0002 -- 0.6379)  max mem: 461
Val:  [ 910/1033]  eta: 0:00:08  loss: 2.3786 (2.4723)  acc1: 41.6667 (42.3802)  acc5: 75.0000 (70.5543)  time: 0.0475 (0.0114 -- 0.3432)  data: 0.0310 (0.0001 -- 0.3265)  max mem: 461
Val:  [ 920/1033]  eta: 0:00:08  loss: 2.4461 (2.4709)  acc1: 37.5000 (42.3362)  acc5: 70.8333 (70.6026)  time: 0.0377 (0.0114 -- 0.3043)  data: 0.0214 (0.0001 -- 0.2849)  max mem: 461
Val:  [ 930/1033]  eta: 0:00:07  loss: 2.4461 (2.4710)  acc1: 41.6667 (42.3469)  acc5: 75.0000 (70.6319)  time: 0.0294 (0.0147 -- 0.1541)  data: 0.0117 (0.0001 -- 0.1349)  max mem: 461
Val:  [ 940/1033]  eta: 0:00:06  loss: 2.4721 (2.4689)  acc1: 41.6667 (42.3928)  acc5: 75.0000 (70.6739)  time: 0.0221 (0.0140 -- 0.1096)  data: 0.0051 (0.0001 -- 0.0936)  max mem: 461
Val:  [ 950/1033]  eta: 0:00:05  loss: 2.2851 (2.4681)  acc1: 45.8333 (42.3896)  acc5: 75.0000 (70.6975)  time: 0.0213 (0.0093 -- 0.0793)  data: 0.0038 (0.0001 -- 0.0660)  max mem: 461
Val:  [ 960/1033]  eta: 0:00:05  loss: 2.3192 (2.4685)  acc1: 45.8333 (42.3691)  acc5: 70.8333 (70.6989)  time: 0.0522 (0.0093 -- 0.5717)  data: 0.0310 (0.0001 -- 0.5357)  max mem: 461
Val:  [ 970/1033]  eta: 0:00:04  loss: 2.4603 (2.4686)  acc1: 41.6667 (42.3790)  acc5: 70.8333 (70.6831)  time: 0.0735 (0.0149 -- 0.5717)  data: 0.0518 (0.0002 -- 0.5357)  max mem: 461
Val:  [ 980/1033]  eta: 0:00:03  loss: 2.6017 (2.4691)  acc1: 37.5000 (42.3505)  acc5: 70.8333 (70.6804)  time: 0.0786 (0.0144 -- 0.6873)  data: 0.0580 (0.0002 -- 0.6662)  max mem: 461
Val:  [ 990/1033]  eta: 0:00:02  loss: 2.5778 (2.4694)  acc1: 41.6667 (42.3983)  acc5: 70.8333 (70.6694)  time: 0.0784 (0.0129 -- 0.6873)  data: 0.0558 (0.0002 -- 0.6662)  max mem: 461
Val:  [1000/1033]  eta: 0:00:02  loss: 2.5035 (2.4718)  acc1: 41.6667 (42.3701)  acc5: 70.8333 (70.6335)  time: 0.0433 (0.0129 -- 0.4399)  data: 0.0224 (0.0001 -- 0.4229)  max mem: 461
Val:  [1010/1033]  eta: 0:00:01  loss: 2.5103 (2.4714)  acc1: 41.6667 (42.3920)  acc5: 66.6667 (70.6190)  time: 0.0445 (0.0136 -- 0.5119)  data: 0.0256 (0.0001 -- 0.4935)  max mem: 461
Val:  [1020/1033]  eta: 0:00:00  loss: 2.4365 (2.4734)  acc1: 41.6667 (42.3441)  acc5: 66.6667 (70.5681)  time: 0.0737 (0.0136 -- 0.6121)  data: 0.0552 (0.0001 -- 0.5951)  max mem: 461
Val:  [1030/1033]  eta: 0:00:00  loss: 2.4384 (2.4731)  acc1: 41.6667 (42.3618)  acc5: 70.8333 (70.5909)  time: 0.0659 (0.0117 -- 0.6121)  data: 0.0494 (0.0001 -- 0.5951)  max mem: 461
Val:  [1032/1033]  eta: 0:00:00  loss: 2.4384 (2.4723)  acc1: 41.6667 (42.3780)  acc5: 70.8333 (70.6018)  time: 0.0679 (0.0108 -- 0.6121)  data: 0.0494 (0.0001 -- 0.5951)  max mem: 461
Val: Total time: 0:01:11 (0.0693 s / it)
* Acc@1 42.378 Acc@5 70.602 loss 2.472
24777 val images: Top-1 42.38%, Top-5 70.60%, loss 2.4723
