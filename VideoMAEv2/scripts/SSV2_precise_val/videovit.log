+ unset SLURM_PROCID
+ export MASTER_PORT=15620
+ MASTER_PORT=15620
+ export OMP_NUM_THREADS=1
+ OMP_NUM_THREADS=1
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
+ OUTPUT_DIR=local/SSV2/videovit
+ DATA_PATH=../dataset/shared_list/SSV2_Mine
+ python3 -m torch.distributed.launch --nproc_per_node=1 --master_port=15620 --nnodes=1 run_class_finetuning.py --model vit_super_tiny_patch8_112 --finetune local/video_mae_super_tiny_pt/checkpoint-299.pth --data_set SSV2 --nb_classes 174 --data_root /localdisk2/dataset/mmdataset --data_path ../dataset/shared_list/SSV2_Mine --log_dir local/SSV2/videovit --output_dir local/SSV2/videovit --batch_size 16 --num_sample 2 --input_size 112 --short_side_size 112 --save_ckpt_freq 10 --num_frames 16 --opt adamw --lr 2e-3 --num_workers 10 --opt_betas 0.9 0.999 --layer_decay 0.65 --weight_decay 0.05 --epochs 50 --test_num_segment 2 --test_num_crop 3 --local-rank 0 --update_freq 8 --warmup_epochs 8 --validation
/opt/conda/lib/python3.10/site-packages/torch/distributed/launch.py:183: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
[2024-11-17 05:26:27,838] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
| distributed init (rank 0): env://, gpu 0
Namespace(batch_size=16, epochs=50, update_freq=8, save_ckpt_freq=10, model='vit_super_tiny_patch8_112', tubelet_size=2, input_size=112, with_checkpoint=False, drop=0.0, attn_drop_rate=0.0, drop_path=0.1, head_drop_rate=0.0, disable_eval_during_finetuning=False, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=[0.9, 0.999], clip_grad=None, momentum=0.9, weight_decay=0.05, weight_decay_end=None, lr=0.002, layer_decay=0.65, warmup_lr=1e-08, min_lr=1e-06, warmup_epochs=8, warmup_steps=-1, color_jitter=0.4, num_sample=2, aa='rand-m7-n4-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', crop_pct=None, short_side_size=112, test_num_segment=2, test_num_crop=3, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='local/video_mae_super_tiny_pt/checkpoint-299.pth', model_key='model|module', model_prefix='', init_scale=0.001, use_mean_pooling=True, data_path='../dataset/shared_list/SSV2_Mine', data_root='/localdisk2/dataset/mmdataset', eval_data_path=None, nb_classes=174, imagenet_default_mean_and_std=True, num_segments=1, num_frames=16, sampling_rate=4, sparse_sample=False, data_set='SSV2', fname_tmpl='img_{:05}.jpg', start_idx=1, output_dir='local/SSV2/videovit', log_dir='local/SSV2/videovit', device='cuda', seed=0, resume='', auto_resume=True, save_ckpt=True, start_epoch=0, eval=False, validation=True, dist_eval=False, num_workers=10, pin_mem=True, world_size=1, dist_on_itp=False, dist_url='env://', enable_deepspeed=False, debug=False, local_rank=0, rank=0, gpu=0, distributed=True, dist_backend='nccl')
Number of the class = 174
Number of the class = 174
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f86b755b460>
Mixup is activated!
Patch size = (8, 8)
Load ckpt from local/video_mae_super_tiny_pt/checkpoint-299.pth
Load state_dict by model_key = model
Weights of VisionTransformer not initialized from pretrained model: ['fc_norm.weight', 'fc_norm.bias', 'head.weight', 'head.bias']
Weights from pretrained model not used in VisionTransformer: ['mask_token', 'decoder.blocks.0.norm1.weight', 'decoder.blocks.0.norm1.bias', 'decoder.blocks.0.attn.q_bias', 'decoder.blocks.0.attn.v_bias', 'decoder.blocks.0.attn.qkv.weight', 'decoder.blocks.0.attn.proj.weight', 'decoder.blocks.0.attn.proj.bias', 'decoder.blocks.0.norm2.weight', 'decoder.blocks.0.norm2.bias', 'decoder.blocks.0.mlp.fc1.weight', 'decoder.blocks.0.mlp.fc1.bias', 'decoder.blocks.0.mlp.fc2.weight', 'decoder.blocks.0.mlp.fc2.bias', 'decoder.blocks.1.norm1.weight', 'decoder.blocks.1.norm1.bias', 'decoder.blocks.1.attn.q_bias', 'decoder.blocks.1.attn.v_bias', 'decoder.blocks.1.attn.qkv.weight', 'decoder.blocks.1.attn.proj.weight', 'decoder.blocks.1.attn.proj.bias', 'decoder.blocks.1.norm2.weight', 'decoder.blocks.1.norm2.bias', 'decoder.blocks.1.mlp.fc1.weight', 'decoder.blocks.1.mlp.fc1.bias', 'decoder.blocks.1.mlp.fc2.weight', 'decoder.blocks.1.mlp.fc2.bias', 'decoder.blocks.2.norm1.weight', 'decoder.blocks.2.norm1.bias', 'decoder.blocks.2.attn.q_bias', 'decoder.blocks.2.attn.v_bias', 'decoder.blocks.2.attn.qkv.weight', 'decoder.blocks.2.attn.proj.weight', 'decoder.blocks.2.attn.proj.bias', 'decoder.blocks.2.norm2.weight', 'decoder.blocks.2.norm2.bias', 'decoder.blocks.2.mlp.fc1.weight', 'decoder.blocks.2.mlp.fc1.bias', 'decoder.blocks.2.mlp.fc2.weight', 'decoder.blocks.2.mlp.fc2.bias', 'decoder.blocks.3.norm1.weight', 'decoder.blocks.3.norm1.bias', 'decoder.blocks.3.attn.q_bias', 'decoder.blocks.3.attn.v_bias', 'decoder.blocks.3.attn.qkv.weight', 'decoder.blocks.3.attn.proj.weight', 'decoder.blocks.3.attn.proj.bias', 'decoder.blocks.3.norm2.weight', 'decoder.blocks.3.norm2.bias', 'decoder.blocks.3.mlp.fc1.weight', 'decoder.blocks.3.mlp.fc1.bias', 'decoder.blocks.3.mlp.fc2.weight', 'decoder.blocks.3.mlp.fc2.bias', 'decoder.norm.weight', 'decoder.norm.bias', 'decoder.head.weight', 'decoder.head.bias', 'encoder_to_decoder.weight', 'norm.weight', 'norm.bias']
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv3d(1, 128, kernel_size=(2, 8, 8), stride=(2, 8, 8))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=128, out_features=384, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=128, out_features=128, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=128, out_features=512, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=512, out_features=128, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=128, out_features=384, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=128, out_features=128, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.014285714365541935)
      (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=128, out_features=512, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=512, out_features=128, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=128, out_features=384, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=128, out_features=128, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.02857142873108387)
      (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=128, out_features=512, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=512, out_features=128, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=128, out_features=384, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=128, out_features=128, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.04285714402794838)
      (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=128, out_features=512, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=512, out_features=128, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=128, out_features=384, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=128, out_features=128, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.05714285746216774)
      (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=128, out_features=512, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=512, out_features=128, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=128, out_features=384, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=128, out_features=128, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0714285746216774)
      (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=128, out_features=512, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=512, out_features=128, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=128, out_features=384, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=128, out_features=128, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.08571428805589676)
      (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=128, out_features=512, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=512, out_features=128, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=128, out_features=384, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=128, out_features=128, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.10000000149011612)
      (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=128, out_features=512, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=512, out_features=128, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): Identity()
  (fc_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
  (head_dropout): Dropout(p=0.0, inplace=False)
  (head): Linear(in_features=128, out_features=174, bias=True)
)
number of params: 1624366
LR = 0.00100000
Batch size = 128
Update frequent = 8
Number of training examples = 168913
Number of training training per epoch = 1319
Assigned values = [0.02071191283789063, 0.03186448128906251, 0.049022278906250015, 0.07541889062500001, 0.11602906250000002, 0.17850625000000003, 0.274625, 0.42250000000000004, 0.65, 1.0]
Skip weight decay list:  {'cls_token', 'pos_embed'}
Param groups = {
  "layer_0_decay": {
    "weight_decay": 0.05,
    "params": [
      "patch_embed.proj.weight"
    ],
    "lr_scale": 0.02071191283789063
  },
  "layer_0_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "patch_embed.proj.bias"
    ],
    "lr_scale": 0.02071191283789063
  },
  "layer_1_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.0.norm1.weight",
      "blocks.0.norm1.bias",
      "blocks.0.attn.q_bias",
      "blocks.0.attn.v_bias",
      "blocks.0.attn.proj.bias",
      "blocks.0.norm2.weight",
      "blocks.0.norm2.bias",
      "blocks.0.mlp.fc1.bias",
      "blocks.0.mlp.fc2.bias"
    ],
    "lr_scale": 0.03186448128906251
  },
  "layer_1_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.0.attn.qkv.weight",
      "blocks.0.attn.proj.weight",
      "blocks.0.mlp.fc1.weight",
      "blocks.0.mlp.fc2.weight"
    ],
    "lr_scale": 0.03186448128906251
  },
  "layer_2_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.1.norm1.weight",
      "blocks.1.norm1.bias",
      "blocks.1.attn.q_bias",
      "blocks.1.attn.v_bias",
      "blocks.1.attn.proj.bias",
      "blocks.1.norm2.weight",
      "blocks.1.norm2.bias",
      "blocks.1.mlp.fc1.bias",
      "blocks.1.mlp.fc2.bias"
    ],
    "lr_scale": 0.049022278906250015
  },
  "layer_2_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.1.attn.qkv.weight",
      "blocks.1.attn.proj.weight",
      "blocks.1.mlp.fc1.weight",
      "blocks.1.mlp.fc2.weight"
    ],
    "lr_scale": 0.049022278906250015
  },
  "layer_3_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.2.norm1.weight",
      "blocks.2.norm1.bias",
      "blocks.2.attn.q_bias",
      "blocks.2.attn.v_bias",
      "blocks.2.attn.proj.bias",
      "blocks.2.norm2.weight",
      "blocks.2.norm2.bias",
      "blocks.2.mlp.fc1.bias",
      "blocks.2.mlp.fc2.bias"
    ],
    "lr_scale": 0.07541889062500001
  },
  "layer_3_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.2.attn.qkv.weight",
      "blocks.2.attn.proj.weight",
      "blocks.2.mlp.fc1.weight",
      "blocks.2.mlp.fc2.weight"
    ],
    "lr_scale": 0.07541889062500001
  },
  "layer_4_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.3.norm1.weight",
      "blocks.3.norm1.bias",
      "blocks.3.attn.q_bias",
      "blocks.3.attn.v_bias",
      "blocks.3.attn.proj.bias",
      "blocks.3.norm2.weight",
      "blocks.3.norm2.bias",
      "blocks.3.mlp.fc1.bias",
      "blocks.3.mlp.fc2.bias"
    ],
    "lr_scale": 0.11602906250000002
  },
  "layer_4_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.3.attn.qkv.weight",
      "blocks.3.attn.proj.weight",
      "blocks.3.mlp.fc1.weight",
      "blocks.3.mlp.fc2.weight"
    ],
    "lr_scale": 0.11602906250000002
  },
  "layer_5_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.4.norm1.weight",
      "blocks.4.norm1.bias",
      "blocks.4.attn.q_bias",
      "blocks.4.attn.v_bias",
      "blocks.4.attn.proj.bias",
      "blocks.4.norm2.weight",
      "blocks.4.norm2.bias",
      "blocks.4.mlp.fc1.bias",
      "blocks.4.mlp.fc2.bias"
    ],
    "lr_scale": 0.17850625000000003
  },
  "layer_5_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.4.attn.qkv.weight",
      "blocks.4.attn.proj.weight",
      "blocks.4.mlp.fc1.weight",
      "blocks.4.mlp.fc2.weight"
    ],
    "lr_scale": 0.17850625000000003
  },
  "layer_6_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.5.norm1.weight",
      "blocks.5.norm1.bias",
      "blocks.5.attn.q_bias",
      "blocks.5.attn.v_bias",
      "blocks.5.attn.proj.bias",
      "blocks.5.norm2.weight",
      "blocks.5.norm2.bias",
      "blocks.5.mlp.fc1.bias",
      "blocks.5.mlp.fc2.bias"
    ],
    "lr_scale": 0.274625
  },
  "layer_6_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.5.attn.qkv.weight",
      "blocks.5.attn.proj.weight",
      "blocks.5.mlp.fc1.weight",
      "blocks.5.mlp.fc2.weight"
    ],
    "lr_scale": 0.274625
  },
  "layer_7_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.6.norm1.weight",
      "blocks.6.norm1.bias",
      "blocks.6.attn.q_bias",
      "blocks.6.attn.v_bias",
      "blocks.6.attn.proj.bias",
      "blocks.6.norm2.weight",
      "blocks.6.norm2.bias",
      "blocks.6.mlp.fc1.bias",
      "blocks.6.mlp.fc2.bias"
    ],
    "lr_scale": 0.42250000000000004
  },
  "layer_7_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.6.attn.qkv.weight",
      "blocks.6.attn.proj.weight",
      "blocks.6.mlp.fc1.weight",
      "blocks.6.mlp.fc2.weight"
    ],
    "lr_scale": 0.42250000000000004
  },
  "layer_8_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.7.norm1.weight",
      "blocks.7.norm1.bias",
      "blocks.7.attn.q_bias",
      "blocks.7.attn.v_bias",
      "blocks.7.attn.proj.bias",
      "blocks.7.norm2.weight",
      "blocks.7.norm2.bias",
      "blocks.7.mlp.fc1.bias",
      "blocks.7.mlp.fc2.bias"
    ],
    "lr_scale": 0.65
  },
  "layer_8_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.7.attn.qkv.weight",
      "blocks.7.attn.proj.weight",
      "blocks.7.mlp.fc1.weight",
      "blocks.7.mlp.fc2.weight"
    ],
    "lr_scale": 0.65
  },
  "layer_9_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "fc_norm.weight",
      "fc_norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  },
  "layer_9_decay": {
    "weight_decay": 0.05,
    "params": [
      "head.weight"
    ],
    "lr_scale": 1.0
  }
}
optimizer settings: {'lr': 0.001, 'weight_decay': 0.0, 'eps': 1e-08, 'betas': [0.9, 0.999]}
Use step level LR scheduler!
Set warmup steps = 10552
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
criterion = SoftTargetCrossEntropy()
Auto resume checkpoint: local/SSV2/videovit/checkpoint-49.pth
Resume checkpoint local/SSV2/videovit/checkpoint-49.pth
With optim & sched!
Val:  [   0/1033]  eta: 0:37:34  loss: 2.4437 (2.4437)  acc1: 45.8333 (45.8333)  acc5: 66.6667 (66.6667)  time: 2.1829 (2.1829 -- 2.1829)  data: 1.0606 (1.0606 -- 1.0606)  max mem: 813
Val:  [  10/1033]  eta: 0:04:01  loss: 2.5857 (2.5076)  acc1: 41.6667 (41.6667)  acc5: 66.6667 (70.8333)  time: 0.2356 (0.0268 -- 2.1829)  data: 0.0971 (0.0002 -- 1.0606)  max mem: 813
Val:  [  20/1033]  eta: 0:02:30  loss: 2.5514 (2.5260)  acc1: 41.6667 (42.2619)  acc5: 66.6667 (69.8413)  time: 0.0466 (0.0268 -- 0.0578)  data: 0.0006 (0.0002 -- 0.0019)  max mem: 813
Val:  [  30/1033]  eta: 0:01:56  loss: 2.6221 (2.5914)  acc1: 37.5000 (39.7849)  acc5: 66.6667 (68.2796)  time: 0.0504 (0.0292 -- 0.0665)  data: 0.0007 (0.0002 -- 0.0045)  max mem: 813
Val:  [  40/1033]  eta: 0:01:45  loss: 2.6859 (2.6286)  acc1: 33.3333 (38.6179)  acc5: 62.5000 (67.1748)  time: 0.0613 (0.0270 -- 0.1456)  data: 0.0165 (0.0002 -- 0.0931)  max mem: 813
Val:  [  50/1033]  eta: 0:01:41  loss: 2.4366 (2.5709)  acc1: 41.6667 (39.9510)  acc5: 70.8333 (68.6275)  time: 0.0844 (0.0270 -- 0.2177)  data: 0.0423 (0.0004 -- 0.1879)  max mem: 813
Val:  [  60/1033]  eta: 0:01:35  loss: 2.4265 (2.5979)  acc1: 41.6667 (39.0710)  acc5: 70.8333 (68.0328)  time: 0.0832 (0.0280 -- 0.2177)  data: 0.0425 (0.0004 -- 0.1879)  max mem: 813
Val:  [  70/1033]  eta: 0:01:32  loss: 2.4516 (2.5692)  acc1: 37.5000 (39.6127)  acc5: 70.8333 (68.8967)  time: 0.0778 (0.0283 -- 0.2266)  data: 0.0313 (0.0003 -- 0.1762)  max mem: 813
Val:  [  80/1033]  eta: 0:01:29  loss: 2.4205 (2.5564)  acc1: 41.6667 (40.0206)  acc5: 70.8333 (68.9300)  time: 0.0796 (0.0272 -- 0.2789)  data: 0.0350 (0.0002 -- 0.2223)  max mem: 813
Val:  [  90/1033]  eta: 0:01:26  loss: 2.4205 (2.5616)  acc1: 41.6667 (40.4304)  acc5: 66.6667 (68.8187)  time: 0.0774 (0.0272 -- 0.2789)  data: 0.0333 (0.0002 -- 0.2223)  max mem: 813
Val:  [ 100/1033]  eta: 0:01:24  loss: 2.6180 (2.5730)  acc1: 37.5000 (39.8927)  acc5: 66.6667 (68.9356)  time: 0.0789 (0.0294 -- 0.3039)  data: 0.0347 (0.0004 -- 0.2484)  max mem: 813
Val:  [ 110/1033]  eta: 0:01:23  loss: 2.5725 (2.5539)  acc1: 37.5000 (40.1652)  acc5: 70.8333 (69.2568)  time: 0.0836 (0.0294 -- 0.3039)  data: 0.0370 (0.0003 -- 0.2512)  max mem: 813
Val:  [ 120/1033]  eta: 0:01:20  loss: 2.5449 (2.5705)  acc1: 37.5000 (39.9105)  acc5: 66.6667 (68.8361)  time: 0.0732 (0.0288 -- 0.2848)  data: 0.0288 (0.0002 -- 0.2512)  max mem: 813
Val:  [ 130/1033]  eta: 0:01:19  loss: 2.5491 (2.5611)  acc1: 41.6667 (40.0763)  acc5: 62.5000 (68.7977)  time: 0.0701 (0.0288 -- 0.2265)  data: 0.0274 (0.0002 -- 0.1697)  max mem: 813
Val:  [ 140/1033]  eta: 0:01:17  loss: 2.4881 (2.5673)  acc1: 41.6667 (39.9823)  acc5: 66.6667 (68.6170)  time: 0.0765 (0.0282 -- 0.2410)  data: 0.0325 (0.0002 -- 0.2120)  max mem: 813
Val:  [ 150/1033]  eta: 0:01:16  loss: 2.5245 (2.5668)  acc1: 37.5000 (40.0662)  acc5: 66.6667 (68.5155)  time: 0.0806 (0.0282 -- 0.2434)  data: 0.0377 (0.0002 -- 0.2120)  max mem: 813
Val:  [ 160/1033]  eta: 0:01:14  loss: 2.6708 (2.5751)  acc1: 37.5000 (40.0104)  acc5: 66.6667 (68.2971)  time: 0.0815 (0.0290 -- 0.2434)  data: 0.0395 (0.0003 -- 0.2091)  max mem: 813
Val:  [ 170/1033]  eta: 0:01:13  loss: 2.6769 (2.5757)  acc1: 37.5000 (39.9854)  acc5: 66.6667 (68.3967)  time: 0.0766 (0.0274 -- 0.2761)  data: 0.0385 (0.0001 -- 0.2486)  max mem: 813
Val:  [ 180/1033]  eta: 0:01:12  loss: 2.6769 (2.5746)  acc1: 41.6667 (40.1013)  acc5: 70.8333 (68.3011)  time: 0.0747 (0.0274 -- 0.2761)  data: 0.0322 (0.0001 -- 0.2486)  max mem: 813
Val:  [ 190/1033]  eta: 0:01:10  loss: 2.6780 (2.5757)  acc1: 41.6667 (40.0742)  acc5: 66.6667 (68.3028)  time: 0.0674 (0.0266 -- 0.1985)  data: 0.0279 (0.0001 -- 0.1693)  max mem: 813
Val:  [ 200/1033]  eta: 0:01:09  loss: 2.4778 (2.5693)  acc1: 41.6667 (40.1949)  acc5: 70.8333 (68.3872)  time: 0.0687 (0.0266 -- 0.2277)  data: 0.0239 (0.0001 -- 0.1733)  max mem: 813
Val:  [ 210/1033]  eta: 0:01:07  loss: 2.4609 (2.5633)  acc1: 41.6667 (40.1856)  acc5: 70.8333 (68.6414)  time: 0.0683 (0.0286 -- 0.2937)  data: 0.0230 (0.0003 -- 0.2592)  max mem: 813
Val:  [ 220/1033]  eta: 0:01:07  loss: 2.5094 (2.5660)  acc1: 37.5000 (40.1207)  acc5: 70.8333 (68.6275)  time: 0.0778 (0.0286 -- 0.3022)  data: 0.0294 (0.0003 -- 0.2592)  max mem: 813
Val:  [ 230/1033]  eta: 0:01:05  loss: 2.4592 (2.5609)  acc1: 41.6667 (40.1335)  acc5: 70.8333 (68.7951)  time: 0.0771 (0.0302 -- 0.3022)  data: 0.0304 (0.0003 -- 0.2660)  max mem: 813
Val:  [ 240/1033]  eta: 0:01:05  loss: 2.3662 (2.5540)  acc1: 45.8333 (40.3181)  acc5: 70.8333 (68.8624)  time: 0.0860 (0.0302 -- 0.6136)  data: 0.0425 (0.0002 -- 0.5560)  max mem: 813
Val:  [ 250/1033]  eta: 0:01:04  loss: 2.4160 (2.5510)  acc1: 41.6667 (40.3220)  acc5: 70.8333 (68.8413)  time: 0.0846 (0.0273 -- 0.6136)  data: 0.0402 (0.0002 -- 0.5560)  max mem: 813
Val:  [ 260/1033]  eta: 0:01:03  loss: 2.5365 (2.5562)  acc1: 37.5000 (40.0862)  acc5: 66.6667 (68.5345)  time: 0.0780 (0.0273 -- 0.4999)  data: 0.0349 (0.0003 -- 0.4507)  max mem: 813
Val:  [ 270/1033]  eta: 0:01:02  loss: 2.6597 (2.5608)  acc1: 37.5000 (40.0062)  acc5: 62.5000 (68.3733)  time: 0.0778 (0.0275 -- 0.4999)  data: 0.0353 (0.0003 -- 0.4507)  max mem: 813
Val:  [ 280/1033]  eta: 0:01:01  loss: 2.6482 (2.5673)  acc1: 37.5000 (39.7835)  acc5: 62.5000 (68.2681)  time: 0.0756 (0.0275 -- 0.4126)  data: 0.0311 (0.0003 -- 0.3596)  max mem: 813
Val:  [ 290/1033]  eta: 0:01:00  loss: 2.6916 (2.5694)  acc1: 37.5000 (39.6764)  acc5: 66.6667 (68.1701)  time: 0.0701 (0.0264 -- 0.4126)  data: 0.0280 (0.0003 -- 0.3596)  max mem: 813
Val:  [ 300/1033]  eta: 0:00:59  loss: 2.4983 (2.5697)  acc1: 37.5000 (39.7148)  acc5: 66.6667 (68.1755)  time: 0.0708 (0.0264 -- 0.4727)  data: 0.0319 (0.0004 -- 0.4442)  max mem: 813
Val:  [ 310/1033]  eta: 0:00:58  loss: 2.5645 (2.5747)  acc1: 41.6667 (39.6972)  acc5: 66.6667 (68.0868)  time: 0.0813 (0.0288 -- 0.4727)  data: 0.0377 (0.0002 -- 0.4442)  max mem: 813
Val:  [ 320/1033]  eta: 0:00:57  loss: 2.5820 (2.5724)  acc1: 41.6667 (39.7975)  acc5: 70.8333 (68.1854)  time: 0.0688 (0.0265 -- 0.3610)  data: 0.0316 (0.0002 -- 0.3035)  max mem: 813
Val:  [ 330/1033]  eta: 0:00:56  loss: 2.6949 (2.5764)  acc1: 37.5000 (39.6400)  acc5: 70.8333 (68.2150)  time: 0.0834 (0.0265 -- 0.5673)  data: 0.0424 (0.0003 -- 0.5129)  max mem: 813
Val:  [ 340/1033]  eta: 0:00:55  loss: 2.7075 (2.5784)  acc1: 37.5000 (39.5406)  acc5: 66.6667 (68.1452)  time: 0.0762 (0.0274 -- 0.5673)  data: 0.0350 (0.0003 -- 0.5129)  max mem: 813
Val:  [ 350/1033]  eta: 0:00:55  loss: 2.4502 (2.5723)  acc1: 41.6667 (39.6842)  acc5: 70.8333 (68.2811)  time: 0.0832 (0.0274 -- 0.6336)  data: 0.0381 (0.0003 -- 0.5754)  max mem: 813
Val:  [ 360/1033]  eta: 0:00:53  loss: 2.4382 (2.5695)  acc1: 41.6667 (39.7161)  acc5: 70.8333 (68.2710)  time: 0.0795 (0.0269 -- 0.6336)  data: 0.0293 (0.0002 -- 0.5754)  max mem: 813
Val:  [ 370/1033]  eta: 0:00:53  loss: 2.5511 (2.5690)  acc1: 41.6667 (39.7237)  acc5: 66.6667 (68.2952)  time: 0.0649 (0.0269 -- 0.3671)  data: 0.0159 (0.0002 -- 0.3088)  max mem: 813
Val:  [ 380/1033]  eta: 0:00:52  loss: 2.4760 (2.5656)  acc1: 41.6667 (39.7528)  acc5: 70.8333 (68.3618)  time: 0.0758 (0.0417 -- 0.3671)  data: 0.0218 (0.0003 -- 0.3088)  max mem: 813
Val:  [ 390/1033]  eta: 0:00:51  loss: 2.5179 (2.5668)  acc1: 41.6667 (39.8018)  acc5: 66.6667 (68.2651)  time: 0.0722 (0.0417 -- 0.2863)  data: 0.0190 (0.0003 -- 0.2310)  max mem: 813
Val:  [ 400/1033]  eta: 0:00:50  loss: 2.5179 (2.5624)  acc1: 41.6667 (39.8795)  acc5: 66.6667 (68.3500)  time: 0.0771 (0.0296 -- 0.2863)  data: 0.0255 (0.0003 -- 0.2310)  max mem: 813
Val:  [ 410/1033]  eta: 0:00:49  loss: 2.3742 (2.5575)  acc1: 41.6667 (39.9331)  acc5: 70.8333 (68.4611)  time: 0.0797 (0.0296 -- 0.2514)  data: 0.0293 (0.0003 -- 0.1982)  max mem: 813
Val:  [ 420/1033]  eta: 0:00:49  loss: 2.4816 (2.5535)  acc1: 45.8333 (39.9842)  acc5: 70.8333 (68.4976)  time: 0.0844 (0.0377 -- 0.3138)  data: 0.0313 (0.0003 -- 0.2542)  max mem: 813
Val:  [ 430/1033]  eta: 0:00:48  loss: 2.5440 (2.5532)  acc1: 41.6667 (39.9942)  acc5: 70.8333 (68.5615)  time: 0.0815 (0.0460 -- 0.3138)  data: 0.0243 (0.0003 -- 0.2542)  max mem: 813
Val:  [ 440/1033]  eta: 0:00:47  loss: 2.6213 (2.5520)  acc1: 37.5000 (40.0321)  acc5: 70.8333 (68.6225)  time: 0.0769 (0.0460 -- 0.2445)  data: 0.0163 (0.0003 -- 0.1874)  max mem: 813
Val:  [ 450/1033]  eta: 0:00:46  loss: 2.4549 (2.5494)  acc1: 37.5000 (40.0129)  acc5: 70.8333 (68.7084)  time: 0.0757 (0.0527 -- 0.2028)  data: 0.0147 (0.0003 -- 0.1402)  max mem: 813
Val:  [ 460/1033]  eta: 0:00:45  loss: 2.5046 (2.5523)  acc1: 37.5000 (39.9313)  acc5: 70.8333 (68.6461)  time: 0.0767 (0.0510 -- 0.2028)  data: 0.0194 (0.0003 -- 0.1408)  max mem: 813
Val:  [ 470/1033]  eta: 0:00:44  loss: 2.5127 (2.5495)  acc1: 37.5000 (39.9947)  acc5: 70.8333 (68.7013)  time: 0.0808 (0.0468 -- 0.2647)  data: 0.0266 (0.0003 -- 0.2138)  max mem: 813
Val:  [ 480/1033]  eta: 0:00:44  loss: 2.5237 (2.5488)  acc1: 41.6667 (40.0554)  acc5: 66.6667 (68.7024)  time: 0.0801 (0.0468 -- 0.2647)  data: 0.0245 (0.0004 -- 0.2138)  max mem: 813
Val:  [ 490/1033]  eta: 0:00:43  loss: 2.5513 (2.5501)  acc1: 41.6667 (40.0543)  acc5: 66.6667 (68.6864)  time: 0.0828 (0.0458 -- 0.3842)  data: 0.0258 (0.0003 -- 0.3269)  max mem: 813
Val:  [ 500/1033]  eta: 0:00:42  loss: 2.7240 (2.5527)  acc1: 37.5000 (39.9617)  acc5: 66.6667 (68.6294)  time: 0.0869 (0.0458 -- 0.3842)  data: 0.0308 (0.0003 -- 0.3269)  max mem: 813
Val:  [ 510/1033]  eta: 0:00:41  loss: 2.6230 (2.5514)  acc1: 37.5000 (39.9380)  acc5: 66.6667 (68.6888)  time: 0.0754 (0.0377 -- 0.3326)  data: 0.0222 (0.0003 -- 0.2769)  max mem: 813
Val:  [ 520/1033]  eta: 0:00:41  loss: 2.6122 (2.5529)  acc1: 37.5000 (39.8912)  acc5: 66.6667 (68.6500)  time: 0.0782 (0.0377 -- 0.3864)  data: 0.0250 (0.0003 -- 0.3306)  max mem: 813
Val:  [ 530/1033]  eta: 0:00:40  loss: 2.4633 (2.5507)  acc1: 37.5000 (39.9011)  acc5: 66.6667 (68.6755)  time: 0.0756 (0.0443 -- 0.3864)  data: 0.0205 (0.0003 -- 0.3306)  max mem: 813
Val:  [ 540/1033]  eta: 0:00:39  loss: 2.5717 (2.5545)  acc1: 33.3333 (39.8182)  acc5: 66.6667 (68.5613)  time: 0.0651 (0.0433 -- 0.2210)  data: 0.0122 (0.0003 -- 0.1700)  max mem: 813
Val:  [ 550/1033]  eta: 0:00:38  loss: 2.5811 (2.5514)  acc1: 33.3333 (39.8064)  acc5: 70.8333 (68.6252)  time: 0.0726 (0.0433 -- 0.2464)  data: 0.0186 (0.0003 -- 0.1921)  max mem: 813
Val:  [ 560/1033]  eta: 0:00:37  loss: 2.4508 (2.5500)  acc1: 37.5000 (39.8916)  acc5: 70.8333 (68.6275)  time: 0.0746 (0.0427 -- 0.2464)  data: 0.0199 (0.0002 -- 0.1921)  max mem: 813
Val:  [ 570/1033]  eta: 0:00:36  loss: 2.4679 (2.5508)  acc1: 45.8333 (39.9227)  acc5: 66.6667 (68.6004)  time: 0.0737 (0.0427 -- 0.2345)  data: 0.0189 (0.0002 -- 0.1787)  max mem: 813
Val:  [ 580/1033]  eta: 0:00:35  loss: 2.4679 (2.5512)  acc1: 41.6667 (39.9025)  acc5: 70.8333 (68.5815)  time: 0.0711 (0.0324 -- 0.2329)  data: 0.0179 (0.0003 -- 0.1725)  max mem: 813
Val:  [ 590/1033]  eta: 0:00:34  loss: 2.5211 (2.5540)  acc1: 41.6667 (39.8407)  acc5: 66.6667 (68.5138)  time: 0.0721 (0.0324 -- 0.2826)  data: 0.0203 (0.0003 -- 0.2224)  max mem: 813
Val:  [ 600/1033]  eta: 0:00:34  loss: 2.6922 (2.5546)  acc1: 37.5000 (39.8017)  acc5: 66.6667 (68.5455)  time: 0.0710 (0.0275 -- 0.3284)  data: 0.0246 (0.0002 -- 0.2613)  max mem: 813
Val:  [ 610/1033]  eta: 0:00:33  loss: 2.6847 (2.5552)  acc1: 33.3333 (39.7845)  acc5: 70.8333 (68.5693)  time: 0.0780 (0.0275 -- 0.3284)  data: 0.0276 (0.0002 -- 0.2818)  max mem: 813
Val:  [ 620/1033]  eta: 0:00:32  loss: 2.5971 (2.5556)  acc1: 37.5000 (39.7746)  acc5: 70.8333 (68.5856)  time: 0.0818 (0.0373 -- 0.3274)  data: 0.0246 (0.0003 -- 0.2818)  max mem: 813
Val:  [ 630/1033]  eta: 0:00:31  loss: 2.5878 (2.5554)  acc1: 41.6667 (39.8640)  acc5: 66.6667 (68.5816)  time: 0.0837 (0.0492 -- 0.4155)  data: 0.0283 (0.0003 -- 0.3578)  max mem: 813
Val:  [ 640/1033]  eta: 0:00:30  loss: 2.5520 (2.5550)  acc1: 41.6667 (39.8726)  acc5: 66.6667 (68.5712)  time: 0.0789 (0.0399 -- 0.4155)  data: 0.0232 (0.0003 -- 0.3578)  max mem: 813
Val:  [ 650/1033]  eta: 0:00:30  loss: 2.5520 (2.5547)  acc1: 41.6667 (39.9322)  acc5: 66.6667 (68.5676)  time: 0.0737 (0.0399 -- 0.2907)  data: 0.0168 (0.0004 -- 0.2276)  max mem: 813
Val:  [ 660/1033]  eta: 0:00:29  loss: 2.5316 (2.5559)  acc1: 41.6667 (39.9017)  acc5: 66.6667 (68.5514)  time: 0.0776 (0.0482 -- 0.2907)  data: 0.0179 (0.0002 -- 0.2276)  max mem: 813
Val:  [ 670/1033]  eta: 0:00:28  loss: 2.4515 (2.5532)  acc1: 41.6667 (39.9218)  acc5: 70.8333 (68.6227)  time: 0.0671 (0.0436 -- 0.1731)  data: 0.0101 (0.0002 -- 0.1143)  max mem: 813
Val:  [ 680/1033]  eta: 0:00:27  loss: 2.4515 (2.5537)  acc1: 41.6667 (39.9168)  acc5: 70.8333 (68.6368)  time: 0.0691 (0.0376 -- 0.2494)  data: 0.0136 (0.0003 -- 0.1864)  max mem: 813
Val:  [ 690/1033]  eta: 0:00:26  loss: 2.5786 (2.5556)  acc1: 37.5000 (39.8456)  acc5: 66.6667 (68.5962)  time: 0.0699 (0.0376 -- 0.2494)  data: 0.0149 (0.0002 -- 0.1864)  max mem: 813
Val:  [ 700/1033]  eta: 0:00:26  loss: 2.5786 (2.5558)  acc1: 37.5000 (39.8122)  acc5: 62.5000 (68.5806)  time: 0.0741 (0.0481 -- 0.3394)  data: 0.0198 (0.0002 -- 0.2848)  max mem: 813
Val:  [ 710/1033]  eta: 0:00:25  loss: 2.4923 (2.5559)  acc1: 37.5000 (39.8383)  acc5: 66.6667 (68.5830)  time: 0.0725 (0.0370 -- 0.3394)  data: 0.0180 (0.0002 -- 0.2848)  max mem: 813
Val:  [ 720/1033]  eta: 0:00:24  loss: 2.5488 (2.5562)  acc1: 41.6667 (39.8000)  acc5: 70.8333 (68.6084)  time: 0.0709 (0.0370 -- 0.2646)  data: 0.0170 (0.0003 -- 0.2193)  max mem: 813
Val:  [ 730/1033]  eta: 0:00:23  loss: 2.5281 (2.5555)  acc1: 41.6667 (39.8199)  acc5: 70.8333 (68.6104)  time: 0.0774 (0.0415 -- 0.2646)  data: 0.0242 (0.0003 -- 0.2193)  max mem: 813
Val:  [ 740/1033]  eta: 0:00:22  loss: 2.5281 (2.5581)  acc1: 41.6667 (39.7998)  acc5: 66.6667 (68.5729)  time: 0.0752 (0.0415 -- 0.2174)  data: 0.0232 (0.0004 -- 0.1517)  max mem: 813
Val:  [ 750/1033]  eta: 0:00:22  loss: 2.6079 (2.5582)  acc1: 41.6667 (39.8247)  acc5: 66.6667 (68.5697)  time: 0.0749 (0.0426 -- 0.2174)  data: 0.0214 (0.0004 -- 0.1517)  max mem: 813
Val:  [ 760/1033]  eta: 0:00:21  loss: 2.6079 (2.5587)  acc1: 41.6667 (39.8653)  acc5: 70.8333 (68.5337)  time: 0.0812 (0.0396 -- 0.3373)  data: 0.0276 (0.0003 -- 0.2803)  max mem: 813
Val:  [ 770/1033]  eta: 0:00:20  loss: 2.5233 (2.5581)  acc1: 41.6667 (39.8562)  acc5: 66.6667 (68.5419)  time: 0.0881 (0.0396 -- 0.3373)  data: 0.0336 (0.0003 -- 0.2803)  max mem: 813
Val:  [ 780/1033]  eta: 0:00:19  loss: 2.5761 (2.5603)  acc1: 37.5000 (39.7941)  acc5: 66.6667 (68.4859)  time: 0.0895 (0.0445 -- 0.4046)  data: 0.0337 (0.0004 -- 0.3465)  max mem: 813
Val:  [ 790/1033]  eta: 0:00:19  loss: 2.6475 (2.5590)  acc1: 33.3333 (39.8125)  acc5: 66.6667 (68.5156)  time: 0.0839 (0.0276 -- 0.4046)  data: 0.0318 (0.0003 -- 0.3465)  max mem: 813
Val:  [ 800/1033]  eta: 0:00:18  loss: 2.4471 (2.5584)  acc1: 37.5000 (39.8200)  acc5: 70.8333 (68.5653)  time: 0.0791 (0.0276 -- 0.3318)  data: 0.0271 (0.0002 -- 0.2766)  max mem: 813
Val:  [ 810/1033]  eta: 0:00:17  loss: 2.4471 (2.5576)  acc1: 37.5000 (39.8377)  acc5: 70.8333 (68.6344)  time: 0.0810 (0.0290 -- 0.3318)  data: 0.0273 (0.0002 -- 0.2766)  max mem: 813
Val:  [ 820/1033]  eta: 0:00:16  loss: 2.5343 (2.5592)  acc1: 37.5000 (39.7889)  acc5: 66.6667 (68.5698)  time: 0.0833 (0.0290 -- 0.3980)  data: 0.0296 (0.0002 -- 0.3235)  max mem: 813
Val:  [ 830/1033]  eta: 0:00:15  loss: 2.5817 (2.5587)  acc1: 37.5000 (39.8115)  acc5: 62.5000 (68.5770)  time: 0.0810 (0.0283 -- 0.3980)  data: 0.0299 (0.0002 -- 0.3235)  max mem: 813
Val:  [ 840/1033]  eta: 0:00:15  loss: 2.4001 (2.5572)  acc1: 45.8333 (39.8236)  acc5: 70.8333 (68.6088)  time: 0.0693 (0.0283 -- 0.3165)  data: 0.0181 (0.0002 -- 0.2597)  max mem: 813
Val:  [ 850/1033]  eta: 0:00:14  loss: 2.3746 (2.5543)  acc1: 41.6667 (39.8502)  acc5: 70.8333 (68.6888)  time: 0.0747 (0.0413 -- 0.2871)  data: 0.0200 (0.0003 -- 0.2424)  max mem: 813
Val:  [ 860/1033]  eta: 0:00:13  loss: 2.4167 (2.5548)  acc1: 37.5000 (39.8180)  acc5: 70.8333 (68.6702)  time: 0.0841 (0.0406 -- 0.2871)  data: 0.0305 (0.0003 -- 0.2424)  max mem: 813
Val:  [ 870/1033]  eta: 0:00:12  loss: 2.6062 (2.5560)  acc1: 37.5000 (39.7723)  acc5: 66.6667 (68.6519)  time: 0.0731 (0.0278 -- 0.2501)  data: 0.0238 (0.0003 -- 0.1899)  max mem: 813
Val:  [ 880/1033]  eta: 0:00:11  loss: 2.5815 (2.5563)  acc1: 37.5000 (39.7654)  acc5: 66.6667 (68.6341)  time: 0.0688 (0.0278 -- 0.2001)  data: 0.0227 (0.0003 -- 0.1553)  max mem: 813
Val:  [ 890/1033]  eta: 0:00:11  loss: 2.5342 (2.5566)  acc1: 37.5000 (39.7681)  acc5: 66.6667 (68.6167)  time: 0.0769 (0.0274 -- 0.2643)  data: 0.0310 (0.0003 -- 0.2068)  max mem: 813
Val:  [ 900/1033]  eta: 0:00:10  loss: 2.4394 (2.5551)  acc1: 37.5000 (39.7937)  acc5: 70.8333 (68.6413)  time: 0.0741 (0.0274 -- 0.2643)  data: 0.0271 (0.0003 -- 0.2068)  max mem: 813
Val:  [ 910/1033]  eta: 0:00:09  loss: 2.4398 (2.5555)  acc1: 37.5000 (39.7777)  acc5: 70.8333 (68.6608)  time: 0.0806 (0.0285 -- 0.2975)  data: 0.0297 (0.0003 -- 0.2466)  max mem: 813
Val:  [ 920/1033]  eta: 0:00:08  loss: 2.5381 (2.5535)  acc1: 37.5000 (39.8208)  acc5: 75.0000 (68.7568)  time: 0.0881 (0.0337 -- 0.3571)  data: 0.0350 (0.0003 -- 0.3030)  max mem: 813
Val:  [ 930/1033]  eta: 0:00:08  loss: 2.3925 (2.5525)  acc1: 41.6667 (39.8272)  acc5: 75.0000 (68.8283)  time: 0.0773 (0.0273 -- 0.3571)  data: 0.0249 (0.0002 -- 0.3030)  max mem: 813
Val:  [ 940/1033]  eta: 0:00:07  loss: 2.4026 (2.5524)  acc1: 37.5000 (39.8202)  acc5: 75.0000 (68.8541)  time: 0.0761 (0.0271 -- 0.3111)  data: 0.0269 (0.0002 -- 0.2538)  max mem: 813
Val:  [ 950/1033]  eta: 0:00:06  loss: 2.4608 (2.5516)  acc1: 37.5000 (39.7608)  acc5: 70.8333 (68.8749)  time: 0.0790 (0.0270 -- 0.3111)  data: 0.0298 (0.0002 -- 0.2538)  max mem: 813
Val:  [ 960/1033]  eta: 0:00:05  loss: 2.4297 (2.5499)  acc1: 37.5000 (39.7849)  acc5: 70.8333 (68.8996)  time: 0.0847 (0.0270 -- 0.4579)  data: 0.0407 (0.0003 -- 0.4030)  max mem: 813
Val:  [ 970/1033]  eta: 0:00:04  loss: 2.4325 (2.5502)  acc1: 37.5000 (39.7614)  acc5: 66.6667 (68.8766)  time: 0.0839 (0.0271 -- 0.4579)  data: 0.0401 (0.0003 -- 0.4030)  max mem: 813
Val:  [ 980/1033]  eta: 0:00:04  loss: 2.5638 (2.5497)  acc1: 37.5000 (39.7893)  acc5: 70.8333 (68.9050)  time: 0.0706 (0.0289 -- 0.2826)  data: 0.0263 (0.0002 -- 0.2512)  max mem: 813
Val:  [ 990/1033]  eta: 0:00:03  loss: 2.5043 (2.5504)  acc1: 37.5000 (39.7788)  acc5: 70.8333 (68.8993)  time: 0.0706 (0.0300 -- 0.2826)  data: 0.0252 (0.0002 -- 0.2512)  max mem: 813
Val:  [1000/1033]  eta: 0:00:02  loss: 2.7079 (2.5522)  acc1: 33.3333 (39.7478)  acc5: 66.6667 (68.8811)  time: 0.0722 (0.0321 -- 0.2929)  data: 0.0231 (0.0002 -- 0.2343)  max mem: 813
Val:  [1010/1033]  eta: 0:00:01  loss: 2.5773 (2.5511)  acc1: 37.5000 (39.7956)  acc5: 66.6667 (68.9210)  time: 0.0822 (0.0321 -- 0.3680)  data: 0.0318 (0.0002 -- 0.3076)  max mem: 813
Val:  [1020/1033]  eta: 0:00:01  loss: 2.3801 (2.5508)  acc1: 41.6667 (39.8057)  acc5: 75.0000 (68.9357)  time: 0.0754 (0.0306 -- 0.3680)  data: 0.0253 (0.0002 -- 0.3076)  max mem: 813
Val:  [1030/1033]  eta: 0:00:00  loss: 2.4082 (2.5495)  acc1: 41.6667 (39.8319)  acc5: 70.8333 (68.9460)  time: 0.0594 (0.0306 -- 0.1795)  data: 0.0124 (0.0001 -- 0.1264)  max mem: 813
Val:  [1032/1033]  eta: 0:00:00  loss: 2.4082 (2.5488)  acc1: 41.6667 (39.8434)  acc5: 70.8333 (68.9510)  time: 0.0593 (0.0306 -- 0.1795)  data: 0.0124 (0.0001 -- 0.1264)  max mem: 813
Val: Total time: 0:01:20 (0.0782 s / it)
* Acc@1 39.843 Acc@5 68.951 loss 2.549
24777 val images: Top-1 39.84%, Top-5 68.95%, loss 2.5488
